<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ESPnet源码解析(二)asr_train.py</title>
    <link href="/2022/04/17/ESPnet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-asr-train-py/"/>
    <url>/2022/04/17/ESPnet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%BA%8C-asr-train-py/</url>
    
    <content type="html"><![CDATA[<p>这部分的代码是声学模型训练的代码的第一部分，以前看的，我代码能力比较薄弱吧，反正只能慢慢改了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br></pre></div></td><td class="code"><pre><code class="hljs plain">#!/usr/bin/env python3<br># encoding: utf-8<br><br># Copyright 2017 Tomoki Hayashi (Nagoya University)<br>#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)<br><br>&quot;&quot;&quot;Automatic speech recognition model training script.&quot;&quot;&quot;<br><br>import logging<br>import os<br>import random<br>import subprocess<br>import sys<br><br>import configargparse<br>import numpy as np<br><br>from espnet import __version__<br>from espnet.utils.cli_utils import strtobool<br>from espnet.utils.training.batchfy import BATCH_COUNT_CHOICES<br><br># NOTE: you need this func to generate our sphinx doc<br>def get_parser(parser=None, required=True):<br>    &quot;&quot;&quot;Get default arguments.&quot;&quot;&quot;<br>    if parser is None:<br>        parser = configargparse.ArgumentParser(<br>            description=&quot;Train an automatic speech recognition (ASR) model on one CPU, &quot;<br>            &quot;one or multiple GPUs&quot;,<br>            config_file_parser_class=configargparse.YAMLConfigFileParser,<br>            formatter_class=configargparse.ArgumentDefaultsHelpFormatter,<br>        )<br>    # general configuration<br>    parser.add(&quot;--config&quot;, is_config_file=True, help=&quot;config file path&quot;)<br>#(config配置文件)<br>    parser.add(<br>        &quot;--config2&quot;,<br>        is_config_file=True,<br>        help=&quot;second config file path that overwrites the settings in `--config`.&quot;,<br>    )<br>    parser.add(<br>        &quot;--config3&quot;,<br>        is_config_file=True,<br>        help=&quot;third config file path that overwrites the settings in &quot;<br>        &quot;`--config` and `--config2`.&quot;,<br>    )<br><br>    parser.add_argument(<br>        &quot;--ngpu&quot;,<br>        default=None,<br>        type=int,<br>        help=&quot;Number of GPUs. If not given, use all visible devices&quot;,<br>    )<br>#gpu使用数量<br>    parser.add_argument(<br>        &quot;--train-dtype&quot;,<br>        default=&quot;float32&quot;,<br>        choices=[&quot;float16&quot;, &quot;float32&quot;, &quot;float64&quot;, &quot;O0&quot;, &quot;O1&quot;, &quot;O2&quot;, &quot;O3&quot;],<br>        help=&quot;Data type for training (only pytorch backend). &quot;<br>        &quot;O0,O1,.. flags require apex. &quot;<br>        &quot;See https://nvidia.github.io/apex/amp.html#opt-levels&quot;,<br>    )<br>#训练数据类型<br>    parser.add_argument(<br>        &quot;--backend&quot;,<br>        default=&quot;chainer&quot;,<br>        type=str,<br>        choices=[&quot;chainer&quot;, &quot;pytorch&quot;],<br>        help=&quot;Backend library&quot;,<br>    )<br>#指定训练框架，pytorch或者chainer<br>    parser.add_argument(<br>        &quot;--outdir&quot;, type=str, required=required, help=&quot;Output directory&quot;<br>    )<br>#输出目录的文件夹<br>    parser.add_argument(&quot;--debugmode&quot;, default=1, type=int, help=&quot;Debugmode&quot;)<br>#debugmode的数量<br>    parser.add_argument(&quot;--dict&quot;, required=required, help=&quot;Dictionary&quot;)<br>#字典<br>    parser.add_argument(&quot;--seed&quot;, default=1, type=int, help=&quot;Random seed&quot;)<br>#随机速度（不知道什么意思）<br>    parser.add_argument(&quot;--debugdir&quot;, type=str, help=&quot;Output directory for debugging&quot;)<br>#debug的输出目录<br>    parser.add_argument(<br>        &quot;--resume&quot;,<br>        &quot;-r&quot;,<br>        default=&quot;&quot;,<br>        nargs=&quot;?&quot;,<br>        help=&quot;Resume the training from snapshot&quot;,<br>    )<br>#从指定的已训练好的模型继续训练<br>    parser.add_argument(<br>        &quot;--minibatches&quot;,<br>        &quot;-N&quot;,<br>        type=int,<br>        default=&quot;-1&quot;,<br>        help=&quot;Process only N minibatches (for debug)&quot;,<br>    )<br>#小批量训练，详细可以百度<br>    parser.add_argument(&quot;--verbose&quot;, &quot;-V&quot;, default=0, type=int, help=&quot;Verbose option&quot;)<br>#日志选项<br>    parser.add_argument(<br>        &quot;--tensorboard-dir&quot;,<br>        default=None,<br>        type=str,<br>        nargs=&quot;?&quot;,<br>        help=&quot;Tensorboard log dir path&quot;,<br>    )<br>#Tensorboard日志目录存放路径，可以学一学<br>    parser.add_argument(<br>        &quot;--report-interval-iters&quot;,<br>        default=100,<br>        type=int,<br>        help=&quot;Report interval iterations&quot;,<br>    )<br>#迭代多少次输出一次，默认100 <br>    parser.add_argument(<br>        &quot;--save-interval-iters&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Save snapshot interval iterations&quot;,<br>    )<br>#保存模型训练的初始迭代<br>    # task related<br>    parser.add_argument(<br>        &quot;--train-json&quot;,<br>        type=str,<br>        default=None,<br>        help=&quot;Filename of train label data (json)&quot;,<br>    )<br>#训练数据类型-jaon<br>    parser.add_argument(<br>        &quot;--valid-json&quot;,<br>        type=str,<br>        default=None,<br>        help=&quot;Filename of validation label data (json)&quot;,<br>    )<br>#验证集数据类型-jaon<br>    # network architecture<br>    parser.add_argument(<br>        &quot;--model-module&quot;,<br>        type=str,<br>        default=None,<br>        help=&quot;model defined module (default: espnet.nets.xxx_backend.e2e_asr:E2E)&quot;,<br>    )<br>#模型定义模块，用pytorch的还是chainer的<br>    # encoder<br>    parser.add_argument(<br>        &quot;--num-encs&quot;, default=1, type=int, help=&quot;Number of encoders in the model.&quot;<br>    )<br>#模型的编码器数量<br>    # loss related<br>    parser.add_argument(<br>        &quot;--ctc_type&quot;,<br>        default=&quot;warpctc&quot;,<br>        type=str,<br>        choices=[&quot;builtin&quot;, &quot;warpctc&quot;, &quot;gtnctc&quot;, &quot;cudnnctc&quot;],<br>        help=&quot;Type of CTC implementation to calculate loss.&quot;,<br>    )<br>#计算CTC的损失是用什么模型实现的，我用的是warpctc<br>    parser.add_argument(<br>        &quot;--mtlalpha&quot;,<br>        default=0.5,<br>        type=float,<br>        help=&quot;Multitask learning coefficient, &quot;<br>        &quot;alpha: alpha*ctc_loss + (1-alpha)*att_loss &quot;,<br>    )<br>#多任务学习系数，公式为上面那个<br>    parser.add_argument(<br>        &quot;--lsm-weight&quot;, default=0.0, type=float, help=&quot;Label smoothing weight&quot;<br>    )<br>#标签平滑权重<br>    # recognition options to compute CER/WER<br>    parser.add_argument(<br>        &quot;--report-cer&quot;,<br>        default=False,<br>        action=&quot;store_true&quot;,<br>        help=&quot;Compute CER on development set&quot;,<br>    )<br>#计算验证集的字错误<br>    parser.add_argument(<br>        &quot;--report-wer&quot;,<br>        default=False,<br>        action=&quot;store_true&quot;,<br>        help=&quot;Compute WER on development set&quot;,<br>    )<br>#计算验证集的词错误率<br>    parser.add_argument(&quot;--nbest&quot;, type=int, default=1, help=&quot;Output N-best hypotheses&quot;)<br>#输出几个最好的假设，默认为1 <br>    parser.add_argument(&quot;--beam-size&quot;, type=int, default=4, help=&quot;Beam size&quot;)<br>#beam search默认设为4<br>    parser.add_argument(&quot;--penalty&quot;, default=0.0, type=float, help=&quot;Incertion penalty&quot;)<br>#插入惩罚参数，默认为0<br>    parser.add_argument(<br>        &quot;--maxlenratio&quot;,<br>        default=0.0,<br>        type=float,<br>        help=&quot;&quot;&quot;Input length ratio to obtain max output length.<br>                        If maxlenratio=0.0 (default), it uses a end-detect function<br>                        to automatically find maximum hypothesis lengths&quot;&quot;&quot;,<br>#输出目标句子最长和句子的最大比，详细看论文混合CTC/attention<br>    )<br>    parser.add_argument(<br>        &quot;--minlenratio&quot;,<br>        default=0.0,<br>        type=float,<br>        help=&quot;Input length ratio to obtain min output length&quot;,<br>    )<br>#同上<br>    parser.add_argument(<br>        &quot;--ctc-weight&quot;, default=0.3, type=float, help=&quot;CTC weight in joint decoding&quot;<br>    )<br>#CTC在联合解码中的权重占比，默认为0.3<br>    parser.add_argument(<br>        &quot;--rnnlm&quot;, type=str, default=None, help=&quot;RNNLM model file to read&quot;<br>    )<br>#RNN语言模型要读取的文件<br>    parser.add_argument(<br>        &quot;--rnnlm-conf&quot;, type=str, default=None, help=&quot;RNNLM model config file to read&quot;<br>    )<br>#RNN语言模型默认配置<br>    parser.add_argument(&quot;--lm-weight&quot;, default=0.1, type=float, help=&quot;RNNLM weight.&quot;)<br>#RNN语言模型在解码中的占比权重<br>    parser.add_argument(&quot;--sym-space&quot;, default=&quot;&lt;space&gt;&quot;, type=str, help=&quot;Space symbol&quot;)<br>#空格符号用&lt;space&gt;代替<br>    parser.add_argument(&quot;--sym-blank&quot;, default=&quot;&lt;blank&gt;&quot;, type=str, help=&quot;Blank symbol&quot;)<br>#空白符号用&lt;blank&gt;符号代替<br>    # minibatch related<br>    parser.add_argument(<br>        &quot;--sortagrad&quot;,<br>        default=0,<br>        type=int,<br>        nargs=&quot;?&quot;,<br>        help=&quot;How many epochs to use sortagrad for. 0 = deactivated, -1 = all epochs&quot;,<br>    )<br>#minibatch相关，多少次epochs遍历完一次<br>    parser.add_argument(<br>        &quot;--batch-count&quot;,<br>        default=&quot;auto&quot;,<br>        choices=BATCH_COUNT_CHOICES,<br>        help=&quot;How to count batch_size. &quot;<br>        &quot;The default (auto) will find how to count by args.&quot;,<br>    )<br>#batchsize大小<br>    parser.add_argument(<br>        &quot;--batch-size&quot;,<br>        &quot;--batch-seqs&quot;,<br>        &quot;-b&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Maximum seqs in a minibatch (0 to disable)&quot;,<br>    )<br>#<br>    parser.add_argument(<br>        &quot;--batch-bins&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Maximum bins in a minibatch (0 to disable)&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--batch-frames-in&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Maximum input frames in a minibatch (0 to disable)&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--batch-frames-out&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Maximum output frames in a minibatch (0 to disable)&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--batch-frames-inout&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Maximum input+output frames in a minibatch (0 to disable)&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--maxlen-in&quot;,<br>        &quot;--batch-seq-maxlen-in&quot;,<br>        default=800,<br>        type=int,<br>        metavar=&quot;ML&quot;,<br>        help=&quot;When --batch-count=seq, &quot;<br>        &quot;batch size is reduced if the input sequence length &gt; ML.&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--maxlen-out&quot;,<br>        &quot;--batch-seq-maxlen-out&quot;,<br>        default=150,<br>        type=int,<br>        metavar=&quot;ML&quot;,<br>        help=&quot;When --batch-count=seq, &quot;<br>        &quot;batch size is reduced if the output sequence length &gt; ML&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--n-iter-processes&quot;,<br>        default=0,<br>        type=int,<br>        help=&quot;Number of processes of iterator&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--preprocess-conf&quot;,<br>        type=str,<br>        default=None,<br>        nargs=&quot;?&quot;,<br>        help=&quot;The configuration file for the pre-processing&quot;,<br>    )<br>    # optimization related<br>    parser.add_argument(<br>        &quot;--opt&quot;,<br>        default=&quot;adadelta&quot;,<br>        type=str,<br>        choices=[&quot;adadelta&quot;, &quot;adam&quot;, &quot;noam&quot;],<br>        help=&quot;Optimizer&quot;,<br>    )<br>#优化模型，&quot;adadelta&quot;, &quot;adam&quot;, &quot;noam&quot;三个选项，默认adadelta<br>    parser.add_argument(<br>        &quot;--accum-grad&quot;, default=1, type=int, help=&quot;Number of gradient accumuration&quot;<br>    )<br>#梯度累计次数<br>    parser.add_argument(<br>        &quot;--eps&quot;, default=1e-8, type=float, help=&quot;Epsilon constant for optimizer&quot;<br>    )<br>#优化器的epsilon系数，因为有的鬼地方防止除0，比如BN<br>    parser.add_argument(<br>        &quot;--eps-decay&quot;, default=0.01, type=float, help=&quot;Decaying ratio of epsilon&quot;<br>    )<br>#优化器epsilon衰减比率<br>    parser.add_argument(<br>        &quot;--weight-decay&quot;, default=0.0, type=float, help=&quot;Weight decay ratio&quot;<br>    )<br>#权重衰减比率<br>    parser.add_argument(<br>        &quot;--criterion&quot;,<br>        default=&quot;acc&quot;,<br>        type=str,<br>        choices=[&quot;loss&quot;, &quot;loss_eps_decay_only&quot;, &quot;acc&quot;],<br>        help=&quot;Criterion to perform epsilon decay&quot;,<br>    )<br>#标准epsilon衰减<br>    parser.add_argument(<br>        &quot;--threshold&quot;, default=1e-4, type=float, help=&quot;Threshold to stop iteration&quot;<br>    )<br>#停止迭代的阈值<br>    parser.add_argument(<br>        &quot;--epochs&quot;, &quot;-e&quot;, default=30, type=int, help=&quot;Maximum number of epochs&quot;<br>    )<br>#epochs次数<br>    parser.add_argument(<br>        &quot;--early-stop-criterion&quot;,<br>        default=&quot;validation/main/acc&quot;,<br>        type=str,<br>        nargs=&quot;?&quot;,<br>        help=&quot;Value to monitor to trigger an early stopping of the training&quot;,<br>    )<br>#监控触发停止训练的值<br>    parser.add_argument(<br>        &quot;--patience&quot;,<br>        default=3,<br>        type=int,<br>        nargs=&quot;?&quot;,<br>        help=&quot;Number of epochs to wait without improvement &quot;<br>        &quot;before stopping the training&quot;,<br>    )<br>#没有再优化模型的epochs的数量，然后提前结束训练<br>    parser.add_argument(<br>        &quot;--grad-clip&quot;, default=5, type=float, help=&quot;Gradient norm threshold to clip&quot;<br>    )<br><br>    parser.add_argument(<br>        &quot;--num-save-attention&quot;,<br>        default=3,<br>        type=int,<br>        help=&quot;Number of samples of attention to be saved&quot;,<br>    )<br>#保留注意力样本的数量，可以看result<br>    parser.add_argument(<br>        &quot;--num-save-ctc&quot;,<br>        default=3,<br>        type=int,<br>        help=&quot;Number of samples of CTC probability to be saved&quot;,<br>    )<br>#要保留ctc概率的数量，也可以看result<br>    parser.add_argument(<br>        &quot;--grad-noise&quot;,<br>        type=strtobool,<br>        default=False,<br>        help=&quot;The flag to switch to use noise injection to gradients during training&quot;,<br>    )<br>#梯度时加噪<br>    # asr_mix related<br>    parser.add_argument(<br>        &quot;--num-spkrs&quot;,<br>        default=1,<br>        type=int,<br>        choices=[1, 2],<br>        help=&quot;Number of speakers in the speech.&quot;,<br>    )<br>#语音中说话人的数量<br>    # decoder related<br>    parser.add_argument(<br>        &quot;--context-residual&quot;,<br>        default=False,<br>        type=strtobool,<br>        nargs=&quot;?&quot;,<br>        help=&quot;The flag to switch to use context vector residual in the decoder network&quot;,<br>    )<br>#使用上下文残差<br>    # finetuning related<br>    parser.add_argument(<br>        &quot;--enc-init&quot;,<br>        default=None,<br>        type=str,<br>        help=&quot;Pre-trained ASR model to initialize encoder.&quot;,<br>    )<br>#预训练语音识别模型的初始化编码器<br>    parser.add_argument(<br>        &quot;--enc-init-mods&quot;,<br>        default=&quot;enc.enc.&quot;,<br>        type=lambda s: [str(mod) for mod in s.split(&quot;,&quot;) if s != &quot;&quot;],<br>        help=&quot;List of encoder modules to initialize, separated by a comma.&quot;,<br>    )<br>#要初始化的编码器模块<br>    parser.add_argument(<br>        &quot;--dec-init&quot;,<br>        default=None,<br>        type=str,<br>        help=&quot;Pre-trained ASR, MT or LM model to initialize decoder.&quot;,<br>    )<br>#预训练语音识别机器翻译和语言模型初始化编码器<br>    parser.add_argument(<br>        &quot;--dec-init-mods&quot;,<br>        default=&quot;att.,dec.&quot;,<br>        type=lambda s: [str(mod) for mod in s.split(&quot;,&quot;) if s != &quot;&quot;],<br>        help=&quot;List of decoder modules to initialize, separated by a comma.&quot;,<br>    )<br>#初始化编码器模块<br>    parser.add_argument(<br>        &quot;--freeze-mods&quot;,<br>        default=None,<br>        type=lambda s: [str(mod) for mod in s.split(&quot;,&quot;) if s != &quot;&quot;],<br>        help=&quot;List of modules to freeze, separated by a comma.&quot;,<br>    )<br>    # front end related<br>    parser.add_argument(<br>        &quot;--use-frontend&quot;,<br>        type=strtobool,<br>        default=False,<br>        help=&quot;The flag to switch to use frontend system.&quot;,<br>    )<br>#这个标志意味着使用前端系统<br>    # WPE related<br>    parser.add_argument(<br>        &quot;--use-wpe&quot;,<br>        type=strtobool,<br>        default=False,<br>        help=&quot;Apply Weighted Prediction Error&quot;,<br>    )<br>#应用权重预测误差，作用是去混响<br>    parser.add_argument(<br>        &quot;--wtype&quot;,<br>        default=&quot;blstmp&quot;,<br>        type=str,<br>        choices=[<br>            &quot;lstm&quot;,<br>            &quot;blstm&quot;,<br>            &quot;lstmp&quot;,<br>            &quot;blstmp&quot;,<br>            &quot;vgglstmp&quot;,<br>            &quot;vggblstmp&quot;,<br>            &quot;vgglstm&quot;,<br>            &quot;vggblstm&quot;,<br>            &quot;gru&quot;,<br>            &quot;bgru&quot;,<br>            &quot;grup&quot;,<br>            &quot;bgrup&quot;,<br>            &quot;vgggrup&quot;,<br>            &quot;vggbgrup&quot;,<br>            &quot;vgggru&quot;,<br>            &quot;vggbgru&quot;,<br>        ],<br>        help=&quot;Type of encoder network architecture &quot;<br>        &quot;of the mask estimator for WPE. &quot;<br>        &quot;&quot;,<br>    )<br>#编码网络类别类别<br>    parser.add_argument(&quot;--wlayers&quot;, type=int, default=2, help=&quot;&quot;)#层数<br>    parser.add_argument(&quot;--wunits&quot;, type=int, default=300, help=&quot;&quot;)#神经元个数<br>    parser.add_argument(&quot;--wprojs&quot;, type=int, default=300, help=&quot;&quot;)<br>    parser.add_argument(&quot;--wdropout-rate&quot;, type=float, default=0.0, help=&quot;&quot;)<br>    parser.add_argument(&quot;--wpe-taps&quot;, type=int, default=5, help=&quot;&quot;)<br>    parser.add_argument(&quot;--wpe-delay&quot;, type=int, default=3, help=&quot;&quot;)<br>    parser.add_argument(<br>        &quot;--use-dnn-mask-for-wpe&quot;,<br>        type=strtobool,<br>        default=False,<br>        help=&quot;Use DNN to estimate the power spectrogram. &quot;<br>        &quot;This option is experimental.&quot;,<br>    )<br>    # Beamformer related<br>    parser.add_argument(&quot;--use-beamformer&quot;, type=strtobool, default=True, help=&quot;&quot;)<br>    parser.add_argument(<br>        &quot;--btype&quot;,<br>        default=&quot;blstmp&quot;,<br>        type=str,<br>        choices=[<br>            &quot;lstm&quot;,<br>            &quot;blstm&quot;,<br>            &quot;lstmp&quot;,<br>            &quot;blstmp&quot;,<br>            &quot;vgglstmp&quot;,<br>            &quot;vggblstmp&quot;,<br>            &quot;vgglstm&quot;,<br>            &quot;vggblstm&quot;,<br>            &quot;gru&quot;,<br>            &quot;bgru&quot;,<br>            &quot;grup&quot;,<br>            &quot;bgrup&quot;,<br>            &quot;vgggrup&quot;,<br>            &quot;vggbgrup&quot;,<br>            &quot;vgggru&quot;,<br>            &quot;vggbgru&quot;,<br>        ],<br>        help=&quot;Type of encoder network architecture &quot;<br>        &quot;of the mask estimator for Beamformer.&quot;,<br>    )<br>    parser.add_argument(&quot;--blayers&quot;, type=int, default=2, help=&quot;&quot;)<br>    parser.add_argument(&quot;--bunits&quot;, type=int, default=300, help=&quot;&quot;)<br>    parser.add_argument(&quot;--bprojs&quot;, type=int, default=300, help=&quot;&quot;)<br>    parser.add_argument(&quot;--badim&quot;, type=int, default=320, help=&quot;&quot;)<br>    parser.add_argument(<br>        &quot;--bnmask&quot;,<br>        type=int,<br>        default=2,<br>        help=&quot;Number of beamforming masks, &quot; &quot;default is 2 for [speech, noise].&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--ref-channel&quot;,<br>        type=int,<br>        default=-1,<br>        help=&quot;The reference channel used for beamformer. &quot;<br>        &quot;By default, the channel is estimated by DNN.&quot;,<br>    )<br>    parser.add_argument(&quot;--bdropout-rate&quot;, type=float, default=0.0, help=&quot;&quot;)<br>    # Feature transform: Normalization<br>    parser.add_argument(<br>        &quot;--stats-file&quot;,<br>        type=str,<br>        default=None,<br>        help=&quot;The stats file for the feature normalization&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--apply-uttmvn&quot;,<br>        type=strtobool,<br>        default=True,<br>        help=&quot;Apply utterance level mean &quot; &quot;variance normalization.&quot;,<br>    )<br>    parser.add_argument(&quot;--uttmvn-norm-means&quot;, type=strtobool, default=True, help=&quot;&quot;)<br>    parser.add_argument(&quot;--uttmvn-norm-vars&quot;, type=strtobool, default=False, help=&quot;&quot;)<br>    # Feature transform: Fbank<br>    parser.add_argument(<br>        &quot;--fbank-fs&quot;,<br>        type=int,<br>        default=16000,<br>        help=&quot;The sample frequency used for &quot; &quot;the mel-fbank creation.&quot;,<br>    )<br>    parser.add_argument(<br>        &quot;--n-mels&quot;, type=int, default=80, help=&quot;The number of mel-frequency bins.&quot;<br>    )<br>    parser.add_argument(&quot;--fbank-fmin&quot;, type=float, default=0.0, help=&quot;&quot;)<br>    parser.add_argument(&quot;--fbank-fmax&quot;, type=float, default=None, help=&quot;&quot;)<br>    return parser<br><br>def main(cmd_args):<br>    &quot;&quot;&quot;Run the main training function.&quot;&quot;&quot;<br>    parser = get_parser()<br>#获取参数<br>    args, _ = parser.parse_known_args(cmd_args)<br>#多次传参<br>    if args.backend == &quot;chainer&quot; and args.train_dtype != &quot;float32&quot;:<br>        raise NotImplementedError(<br>            f&quot;chainer backend does not support --train-dtype &#123;args.train_dtype&#125;.&quot;<br>            &quot;Use --dtype float32.&quot;<br>        )<br>#如果选择chainer框架且训练类型不是float32则报错<br>    if args.ngpu == 0 and args.train_dtype in (&quot;O0&quot;, &quot;O1&quot;, &quot;O2&quot;, &quot;O3&quot;, &quot;float16&quot;):<br>        raise ValueError(<br>            f&quot;--train-dtype &#123;args.train_dtype&#125; does not support the CPU backend.&quot;<br>        )<br>#如果使用CPU训练且训练类型为O0-O3等报错<br>    from espnet.utils.dynamic_import import dynamic_import<br>#从espnet_utils_dynamic_import.py下引用方法dynamic_import，作用动态引入模块<br>    if args.model_module is None:<br>        if args.num_spkrs == 1:<br>            model_module = &quot;espnet.nets.&quot; + args.backend + &quot;_backend.e2e_asr:E2E&quot;<br>        else:<br>            model_module = &quot;espnet.nets.&quot; + args.backend + &quot;_backend.e2e_asr_mix:E2E&quot;<br>    else:<br>        model_module = args.model_module<br>    model_class = dynamic_import(model_module)<br>#代码注释见https://shimo.im/docs/9030MPWRzYUNZrqw,这里是指e2e_asr_transformer.py中的E2E类<br>    model_class.add_arguments(parser)<br>#将parser参数传给模型，详情见https://shimo.im/docs/vVAXVoDOYNFVVjqm<br><br>    args = parser.parse_args(cmd_args)<br>    args.model_module = model_module<br>    if &quot;chainer_backend&quot; in args.model_module:<br>        args.backend = &quot;chainer&quot;<br>    if &quot;pytorch_backend&quot; in args.model_module:<br>        args.backend = &quot;pytorch&quot;<br>#指定框架,这里用的是pytorch<br><br>    # add version info in args<br>    args.version = __version__<br>#添加版本信息<br>    # logging info<br>    if args.verbose &gt; 0:<br>        logging.basicConfig(<br>            level=logging.INFO,<br>            format=&quot;%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s&quot;,<br>        )<br>    else:<br>        logging.basicConfig(<br>            level=logging.WARN,<br>            format=&quot;%(asctime)s (%(module)s:%(lineno)d) %(levelname)s: %(message)s&quot;,<br>        )<br>        logging.warning(&quot;Skip DEBUG/INFO messages&quot;)<br><br>    # If --ngpu is not given,<br>    #   1. if CUDA_VISIBLE_DEVICES is set, all visible devices<br>    #   2. if nvidia-smi exists, use all devices<br>    #   3. else ngpu=0<br>    if args.ngpu is None:<br>        cvd = os.environ.get(&quot;CUDA_VISIBLE_DEVICES&quot;)<br>#获取能用的GPU<br>        if cvd is not None:<br>            ngpu = len(cvd.split(&quot;,&quot;))<br>#如果cvd不为空，按“，”切分，获取gpu数量<br>        else:<br>            logging.warning(&quot;CUDA_VISIBLE_DEVICES is not set.&quot;)<br>            try:<br>                p = subprocess.run(<br>                    [&quot;nvidia-smi&quot;, &quot;-L&quot;], stdout=subprocess.PIPE, stderr=subprocess.PIPE<br>                )<br>            except (subprocess.CalledProcessError, FileNotFoundError):<br>                ngpu = 0<br>#警告GPU没设置<br>            else:<br>                ngpu = len(p.stderr.decode().split(&quot;\n&quot;)) - 1<br>    else:<br>        if args.ngpu != 1:<br>            logging.debug(<br>                &quot;There are some bugs with multi-GPU processing in PyTorch 1.2+&quot;<br>                + &quot; (see https://github.com/pytorch/pytorch/issues/21108)&quot;<br>            )<br>        ngpu = args.ngpu<br>    logging.info(f&quot;ngpu: &#123;ngpu&#125;&quot;)<br>#如果GPU数量不等于1，提出警告需要pytorch1.2+,<br>   # display PYTHONPATH<br>    logging.info(&quot;python path = &quot; + os.environ.get(&quot;PYTHONPATH&quot;, &quot;(None)&quot;))<br>#python路径<br>    # set random seed<br>    logging.info(&quot;random seed = %d&quot; % args.seed)<br>    random.seed(args.seed)<br>    np.random.seed(args.seed)<br>#设置随机seed，就是避免二次调用的时候产生不同的随机数据集。你再问细一点我也不知道<br>    # load dictionary for debug log<br>    if args.dict is not None:<br>        with open(args.dict, &quot;rb&quot;) as f:<br>            dictionary = f.readlines()<br>#如果字典不为空，按行读取一行就是长这样“一 2”，<br>        char_list = [entry.decode(&quot;utf-8&quot;).split(&quot; &quot;)[0] for entry in dictionary]<br>        #前面字符后面数字映射，按空格切分取出字  <br>        char_list.insert(0, &quot;&lt;blank&gt;&quot;)<br>        #在索引为0的位置插入&lt;blank&gt;,就是第一个位置插入&lt;blank&gt;<br>        char_list.append(&quot;&lt;eos&gt;&quot;)<br>        #在最后的位置插入&lt;eos&gt;<br>        # for non-autoregressive maskctc model<br>        if &quot;maskctc&quot; in args.model_module:<br>            char_list.append(&quot;&lt;mask&gt;&quot;)<br>        #参考论文：Mask CTC: Non-Autoregressive End-to-End ASR with CTC and Mask Predict<br>        args.char_list = char_list<br>        #重新将字典赋值给args<br>    else:<br>        args.char_list = None<br><br>    # train<br>    logging.info(&quot;backend = &quot; + args.backend)<br><br>    if args.num_spkrs == 1:<br>        if args.backend == &quot;chainer&quot;:<br>            from espnet.asr.chainer_backend.asr import train<br><br>            train(args)<br>        elif args.backend == &quot;pytorch&quot;:<br>            from espnet.asr.pytorch_backend.asr import train<br><br>            train(args)<br>        else:<br>            raise ValueError(&quot;Only chainer and pytorch are supported.&quot;)<br>    else:<br>        # FIXME(kamo): Support --model-module<br>        if args.backend == &quot;pytorch&quot;:<br>            from espnet.asr.pytorch_backend.asr_mix import train<br><br>            train(args)<br>        else:<br>            raise ValueError(&quot;Only pytorch is supported.&quot;)<br>#训练,详情见https://shimo.im/docs/5xkGMLnEE9cQxp3X<br>if __name__ == &quot;__main__&quot;:<br>    main(sys.argv[1:])<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ESPnet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ESPnet</tag>
      
      <tag>ESPnet_train</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ESPnet源码解析(一)run.sh</title>
    <link href="/2022/04/17/ESPnet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-run-sh/"/>
    <url>/2022/04/17/ESPnet%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E4%B8%80-run-sh/</url>
    
    <content type="html"><![CDATA[<p>网上关于ESPnet的相关内容比较少，我也只能靠自己不断摸索总结，这是我对自己看的源码进行的一些记录，应该会有很多错误，希望各位大佬在评论区大力斧正！！！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br></pre></div></td><td class="code"><pre><code class="hljs plain">2021.9.17<br>#!/bin/bash<br> <br># Copyright 2017 Johns Hopkins University (Shinji Watanabe)<br>#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)<br> <br>. ./path.sh || exit 1;<br>. ./cmd.sh || exit 1;<br> <br># general configuration<br>backend=pytorch<br>stage=0       # start from 0 if you need to start from data preparation<br>#从步骤0开始<br>stop_stage=100<br>#到步骤100结束<br>ngpu=1       # number of gpus (&quot;0&quot; uses cpu, otherwise use gpu)<br>#gpu的数量设为1<br>debugmode=1<br>dumpdir=dump   # directory to dump full features<br>N=0            # number of minibatches to be used (mainly for debugging). &quot;0&quot; uses all minibatches.<br>verbose=0      # verbose option<br>resume=        # Resume the training from snapshot<br> <br># feature configuration<br>do_delta=false<br> <br>train_config=conf/tuning/train_CTC.yaml<br>#训练设置<br>lm_config=conf/lm.yaml<br>#语言模型设置<br>decode_config=conf/decode.yaml<br>#解码设置<br> <br># rnnlm related<br>lm_resume=         # specify a snapshot file to resume LM training<br>lmtag=             # tag for managing LMs<br> <br># decoding parameter<br>recog_model=model.acc.best # set a model to be used for decoding: &#x27;model.acc.best&#x27; or &#x27;model.loss.best&#x27;<br>n_average=10<br>#recog_model因为用了10条路径，选择正确率最优的路径，注释里说选择损失函数最好的路径<br> <br># data<br>data=./export/data<br>data_url=www.openslr.org/resources/33<br>#data数据存放的目录<br>#data_url数据下载网址<br> <br># exp tag<br>tag=&quot;CTC&quot; # tag for managing experiments.<br>#实验命名，为了区分 建议设置<br> <br>. ./utils/parse_options.sh || exit 1;<br>#shell命令使用parse_options.sh传递<br> <br># Set bash to &#x27;debug&#x27; mode, it will exit on :<br># -e &#x27;error&#x27;, -u &#x27;undefined variable&#x27;, -o ... &#x27;error in pipeline&#x27;, -x &#x27;print commands&#x27;,<br>set -e<br>set -u<br>set -o pipefail<br> <br>train_set=train_sp<br>train_dev=dev<br>recog_set=&quot;dev test&quot;<br> <br>if [ $&#123;stage&#125; -le -1 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge -1 ]; then<br>    echo &quot;stage -1: Data Download&quot;<br>    local/download_and_untar.sh $&#123;data&#125; $&#123;data_url&#125; data_aishell<br>    local/download_and_untar.sh $&#123;data&#125; $&#123;data_url&#125; resource_aishell<br>fi<br>#下载数据集，通过运行download_and_untar.sh脚本下载aishell数据集，下载后存放在$data下<br> <br>#stage0~2: kaldi格式数据集准备<br>#stage0: 数据准备<br>if [ $&#123;stage&#125; -le 0 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge 0 ]; then<br>    ### Task dependent. You have to make data the following preparation part by yourself.<br>    ### But you can utilize Kaldi recipes in most cases<br>    echo &quot;stage 0: Data preparation&quot;<br>    local/data_prep.sh $&#123;data&#125;/data_record/wav $&#123;data&#125;/data_record/transcript<br>    # remove space in text<br>    for x in train dev test; do<br>        cp data/$&#123;x&#125;/text data/$&#123;x&#125;/text.org<br>        paste -d &quot; &quot; &lt;(cut -f 1 -d&quot; &quot; data/$&#123;x&#125;/text.org) &lt;(cut -f 2- -d&quot; &quot; data/$&#123;x&#125;/text.org | tr -d &quot; &quot;) \<br>            &gt; data/$&#123;x&#125;/text<br>        rm data/$&#123;x&#125;/text.org<br>    done<br>fi<br>#上面就是数据准备的工作，目的就是创建和kaldi风格一样的数据格式wav.scp，utt2spk,spk2utt,text<br> <br>feat_tr_dir=$&#123;dumpdir&#125;/$&#123;train_set&#125;/delta$&#123;do_delta&#125;; mkdir -p $&#123;feat_tr_dir&#125;<br>feat_dt_dir=$&#123;dumpdir&#125;/$&#123;train_dev&#125;/delta$&#123;do_delta&#125;; mkdir -p $&#123;feat_dt_dir&#125;<br>#递归创建这些文件夹<br>#stage1：特征提取<br>if [ $&#123;stage&#125; -le 1 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge 1 ]; then<br>    ### Task dependent. You have to design training and dev sets by yourself.<br>    ### But you can utilize Kaldi recipes in most cases<br>    echo &quot;stage 1: Feature Generation&quot;<br>    fbankdir=fbank<br>    # Generate the fbank features; by default 80-dimensional fbanks with pitch on each frame<br>    steps/make_fbank_pitch.sh --cmd &quot;$train_cmd&quot; --nj 32 --write_utt2num_frames true \<br>        data/train exp/make_fbank/train $&#123;fbankdir&#125;<br>    utils/fix_data_dir.sh data/train<br>    steps/make_fbank_pitch.sh --cmd &quot;$train_cmd&quot; --nj 10 --write_utt2num_frames true \<br>        data/dev exp/make_fbank/dev $&#123;fbankdir&#125;<br>    utils/fix_data_dir.sh data/dev<br>    steps/make_fbank_pitch.sh --cmd &quot;$train_cmd&quot; --nj 10 --write_utt2num_frames true \<br>        data/test exp/make_fbank/test $&#123;fbankdir&#125;<br>    utils/fix_data_dir.sh data/test<br>#make_fbank_pitch.sh脚本用来提取80维的fbank特征+3维pitch特征，放在exp/make_fbank/train/fbank文件夹下。<br>#fix_data_dir.sh是检验数据集，删除一些没有任何特征的数据并且保证文件是有序的，节省算力。<br> <br>#数据增强操作：最常用的方法是速度扰动和音量扰动。<br>#速度扰动<br>    # speed-perturbed<br>    utils/perturb_data_dir_speed.sh 0.9 data/train data/temp1<br>    utils/perturb_data_dir_speed.sh 1.0 data/train data/temp2<br>    utils/perturb_data_dir_speed.sh 1.1 data/train data/temp3<br># perturb_data_dir_speed.sh脚本进行速度扰动，就是使原来的语音进行变速，使得训练出来的模型能够适应不同的语音语速。<br>  utils/combine_data.sh --extra-files utt2uniq data/$&#123;train_set&#125; data/temp1 data/temp2 data/temp3<br>#把三种不同语音速度的data进行组合<br>    rm -r data/temp1 data/temp2 data/temp3<br>    steps/make_fbank_pitch.sh --cmd &quot;$train_cmd&quot; --nj 32 --write_utt2num_frames true \<br>        data/$&#123;train_set&#125; exp/make_fbank/$&#123;train_set&#125; $&#123;fbankdir&#125;<br>    utils/fix_data_dir.sh data/$&#123;train_set&#125;<br>#进行语音增强后要重新进行特征提取<br> <br>#倒谱均值归一化：作用使特征服从均值为0，方差为1的高斯分布。具体看kaldi代码中的注释<br>    # compute global CMVN<br>    compute-cmvn-stats scp:data/$&#123;train_set&#125;/feats.scp data/$&#123;train_set&#125;/cmvn.ark<br> <br>#特征转存<br>    # dump features for training<br>    split_dir=$(echo $PWD | awk -F &quot;/&quot; &#x27;&#123;print $NF &quot;/&quot; $(NF-1)&#125;&#x27;)<br>    if [[ $(hostname -f) == *.clsp.jhu.edu ]] &amp;&amp; [ ! -d $&#123;feat_tr_dir&#125;/storage ]; then<br>    utils/create_split_dir.pl \<br>        /export/a&#123;11,12,13,14&#125;/$&#123;USER&#125;/espnet-data/egs/$&#123;split_dir&#125;/dump/$&#123;train_set&#125;/delta$&#123;do_delta&#125;/storage \<br>        $&#123;feat_tr_dir&#125;/storage<br>    fi<br>    if [[ $(hostname -f) == *.clsp.jhu.edu ]] &amp;&amp; [ ! -d $&#123;feat_dt_dir&#125;/storage ]; then<br>    utils/create_split_dir.pl \<br>        /export/a&#123;11,12,13,14&#125;/$&#123;USER&#125;/espnet-data/egs/$&#123;split_dir&#125;/dump/$&#123;train_dev&#125;/delta$&#123;do_delta&#125;/storage \<br>        $&#123;feat_dt_dir&#125;/storage<br>    fi<br>    dump.sh --cmd &quot;$train_cmd&quot; --nj 32 --do_delta $&#123;do_delta&#125; \<br>        data/$&#123;train_set&#125;/feats.scp data/$&#123;train_set&#125;/cmvn.ark exp/dump_feats/train $&#123;feat_tr_dir&#125;<br>    for rtask in $&#123;recog_set&#125;; do<br>        feat_recog_dir=$&#123;dumpdir&#125;/$&#123;rtask&#125;/delta$&#123;do_delta&#125;; mkdir -p $&#123;feat_recog_dir&#125;<br>        dump.sh --cmd &quot;$train_cmd&quot; --nj 10 --do_delta $&#123;do_delta&#125; \<br>            data/$&#123;rtask&#125;/feats.scp data/$&#123;train_set&#125;/cmvn.ark exp/dump_feats/recog/$&#123;rtask&#125; \<br>            $&#123;feat_recog_dir&#125;<br>    done<br>fi<br>#<br> <br>#字典准备：因为计算机进行深度学习模型的时，字符不能直接作为输入的，要将字符转换成特定的数字<br>dict=data/lang_1char/$&#123;train_set&#125;_units.txt<br>echo &quot;dictionary: $&#123;dict&#125;&quot;<br>if [ $&#123;stage&#125; -le 2 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge 2 ]; then<br>    ### Task dependent. You have to check non-linguistic symbols used in the corpus.<br>    echo &quot;stage 2: Dictionary and Json Data Preparation&quot;<br>    mkdir -p data/lang_1char/<br>#递归创建字典文件夹<br> <br>    echo &quot;make a dictionary&quot;<br>    echo &quot;&lt;unk&gt; 1&quot; &gt; $&#123;dict&#125; # &lt;unk&gt; must be 1, 0 will be used for &quot;blank&quot; in CTC<br>    text2token.py -s 1 -n 1 data/$&#123;train_set&#125;/text | cut -f 2- -d&quot; &quot; | tr &quot; &quot; &quot;\n&quot; \<br>    | sort | uniq | grep -v -e &#x27;^\s*$&#x27; | awk &#x27;&#123;print $0 &quot; &quot; NR+1&#125;&#x27; &gt;&gt; $&#123;dict&#125;<br>    wc -l $&#123;dict&#125;<br>#espnet中使用text2token.py来通过映射文件中的text文件生成词典.uniq用于去重复<br> <br>    echo &quot;make json files&quot;<br>    data2json.sh --feat $&#123;feat_tr_dir&#125;/feats.scp \<br>data/$&#123;train_set&#125; $&#123;dict&#125; &gt; $&#123;feat_tr_dir&#125;/data.json<br>    for rtask in $&#123;recog_set&#125;; do<br>        feat_recog_dir=$&#123;dumpdir&#125;/$&#123;rtask&#125;/delta$&#123;do_delta&#125;<br>        data2json.sh --feat $&#123;feat_recog_dir&#125;/feats.scp \<br>     data/$&#123;rtask&#125; $&#123;dict&#125; &gt; $&#123;feat_recog_dir&#125;/data.json<br>    done<br>fi<br>#espnet中训练神经网络时，不是直接使用提取的特征和text这些映射文件，而是通过脚本data2json.sh将文件打包成一个json文件。整体结构分为两部分：input和ouput。input为语音的特征以及特征的形状shape;out为语音对应的文本及数字表示。<br> <br># you can skip this and remove --rnnlm option in the recognition (stage 5)<br>if [ -z $&#123;lmtag&#125; ]; then<br>    lmtag=$(basename $&#123;lm_config%.*&#125;)<br>fi<br>lmexpname=train_rnnlm_$&#123;backend&#125;_$&#123;lmtag&#125;<br>lmexpdir=exp/$&#123;lmexpname&#125;<br>mkdir -p $&#123;lmexpdir&#125;<br>#语言模型的训练<br>if [ $&#123;stage&#125; -le 3 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge 3 ]; then<br>    echo &quot;stage 3: LM Preparation&quot;<br>    lmdatadir=data/local/lm_train<br>    mkdir -p $&#123;lmdatadir&#125;<br>    text2token.py -s 1 -n 1 data/train/text | cut -f 2- -d&quot; &quot; \<br>        &gt; $&#123;lmdatadir&#125;/train.txt<br>    text2token.py -s 1 -n 1 data/$&#123;train_dev&#125;/text | cut -f 2- -d&quot; &quot; \<br>        &gt; $&#123;lmdatadir&#125;/valid.txt<br> <br>#使用lm_train.py脚本进行语言模型的训练<br>    $&#123;cuda_cmd&#125; --gpu $&#123;ngpu&#125; $&#123;lmexpdir&#125;/train.log \<br>        lm_train.py \<br>        --config $&#123;lm_config&#125; \<br>        --ngpu $&#123;ngpu&#125; \<br>        --backend $&#123;backend&#125; \<br>        --verbose 1 \<br>        --outdir $&#123;lmexpdir&#125; \<br>        --tensorboard-dir tensorboard/$&#123;lmexpname&#125; \<br>        --train-label $&#123;lmdatadir&#125;/train.txt \<br>        --valid-label $&#123;lmdatadir&#125;/valid.txt \<br>        --resume $&#123;lm_resume&#125; \<br>        --dict $&#123;dict&#125;<br>fi<br>#需要准备4个文件，第一个在conf里面要准备的lm.yaml配置脚本对应lm_config，第二个第三个就是训练集train.txt和验证集文本文件valid.txt，最后一个就是词典了对应dict,将训练集和验证集文本转成数字进行训练。其他的就是ngpu就是训练时用的gpu个数，bakend就是选择网络框架(我们基本都是用pytorch)，verbose表示log信息的输出格式，tensorboard工具进行训练过程监督训练的训练信息存储文件夹，resume表示加载上一次训练结束后保存的模型的路径。outdir是训练完成后语言模型存放的路径。这段代码脚本结束后，得到的是训练完成后语言模型。<br> <br>if [ -z $&#123;tag&#125; ]; then<br>    expname=$&#123;train_set&#125;_$&#123;backend&#125;_$(basename $&#123;train_config%.*&#125;)<br>    if $&#123;do_delta&#125;; then<br>        expname=$&#123;expname&#125;_delta<br>    fi<br>else<br>    expname=$&#123;train_set&#125;_$&#123;backend&#125;_$&#123;tag&#125;<br>fi<br>expdir=exp/$&#123;expname&#125;<br>mkdir -p $&#123;expdir&#125;<br> <br>#stage4：声学模型训练<br>if [ $&#123;stage&#125; -le 4 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge 4 ]; then<br>    echo &quot;stage 4: Network Training&quot;<br>    $&#123;cuda_cmd&#125; --gpu $&#123;ngpu&#125; $&#123;expdir&#125;/train.log \<br>        asr_train.py \<br>        --config $&#123;train_config&#125; \<br>        --ngpu $&#123;ngpu&#125; \<br>        --backend $&#123;backend&#125; \<br>        --outdir $&#123;expdir&#125;/results \<br>        --tensorboard-dir tensorboard/$&#123;expname&#125; \<br>        --debugmode $&#123;debugmode&#125; \<br>        --dict $&#123;dict&#125; \<br>        --debugdir $&#123;expdir&#125; \<br>        --minibatches $&#123;N&#125; \<br>        --verbose $&#123;verbose&#125; \<br>        --resume $&#123;resume&#125; \<br>        --train-json $&#123;feat_tr_dir&#125;/data.json \<br>        --valid-json $&#123;feat_dt_dir&#125;/data.json<br>fi<br>#声学模型的配置：基本和lm训练一样;outdir是训练完后模型存放路径，还有两个data.json文件,分别是训练集和验证集。最重要的就是配置文件train.yaml,可以用来选择不同的模型。<br> <br>#解码<br>if [ $&#123;stage&#125; -le 5 ] &amp;&amp; [ $&#123;stop_stage&#125; -ge 5 ]; then<br>    echo &quot;stage 5: Decoding&quot;<br>    nj=32<br>    if [[ $(get_yaml.py $&#123;train_config&#125; model-module) = *transformer* ]]; then<br>recog_model=model.last$&#123;n_average&#125;.avg.best<br>average_checkpoints.py --backend $&#123;backend&#125; \<br>       --snapshots $&#123;expdir&#125;/results/snapshot.ep.* \<br>       --out $&#123;expdir&#125;/results/$&#123;recog_model&#125; \<br>       --num $&#123;n_average&#125;<br>    fi<br>#以上这一部分如果是transformer,就进行设置<br>    pids=() # initialize pids<br>    for rtask in $&#123;recog_set&#125;; do<br>    (<br>        decode_dir=decode_$&#123;rtask&#125;_$(basename $&#123;decode_config%.*&#125;)_$&#123;lmtag&#125;<br>        feat_recog_dir=$&#123;dumpdir&#125;/$&#123;rtask&#125;/delta$&#123;do_delta&#125;<br> <br>        # split data<br>        splitjson.py --parts $&#123;nj&#125; $&#123;feat_recog_dir&#125;/data.json<br> <br>        #### use CPU for decoding<br>        ngpu=0<br> <br>        $&#123;decode_cmd&#125; JOB=1:$&#123;nj&#125; $&#123;expdir&#125;/$&#123;decode_dir&#125;/log/decode.JOB.log \<br>            asr_recog.py \<br>            --config $&#123;decode_config&#125; \<br>            --ngpu $&#123;ngpu&#125; \<br>            --backend $&#123;backend&#125; \<br>            --batchsize 0 \<br>            --recog-json $&#123;feat_recog_dir&#125;/split$&#123;nj&#125;utt/data.JOB.json \<br>            --result-label $&#123;expdir&#125;/$&#123;decode_dir&#125;/data.JOB.json \<br>            --model $&#123;expdir&#125;/results/$&#123;recog_model&#125;  \<br>            --rnnlm $&#123;lmexpdir&#125;/rnnlm.model.best<br>#解码这一部分和前面也是类似的<br>        score_sclite.sh $&#123;expdir&#125;/$&#123;decode_dir&#125; $&#123;dict&#125;<br> <br>    ) &amp;<br>    pids+=($!) # store background pids<br>    done<br>    i=0; for pid in &quot;$&#123;pids[@]&#125;&quot;; do wait $&#123;pid&#125; || ((++i)); done<br>    [ $&#123;i&#125; -gt 0 ] &amp;&amp; echo &quot;$0: $&#123;i&#125; background jobs are failed.&quot; &amp;&amp; false<br>    echo &quot;Finished&quot;<br>fi<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>ESPnet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ESPnet</tag>
      
      <tag>源码</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ESPnet基础理论(一)总览</title>
    <link href="/2022/04/17/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/"/>
    <url>/2022/04/17/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/</url>
    
    <content type="html"><![CDATA[<p>这是我2021年9月份做的一次组会报告，不知道咋上传ppt，因此以图片上传上来，基本概括了ESPnet涉及到的最基础的理论知识吧，后面我有时间将会将涉及到的CTC、RNNT、Attention等逐一进行讲解，有时间的话，还有一些最新的端到端语音识别知识，比如语言模型的融合，这是最近在看的东西，当然得我学会还有时间才会写啊。</p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%871.PNG" alt="幻灯片1"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%872.PNG" alt="幻灯片2"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%873.PNG" alt="幻灯片3"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%874.PNG" alt="幻灯片4"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%875.PNG" alt="幻灯片5"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%876.PNG" alt="幻灯片6"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%877.PNG" alt="幻灯片7"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%878.PNG" alt="幻灯片8"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%879.PNG" alt="幻灯片9"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8710.PNG" alt="幻灯片10"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8711.PNG" alt="幻灯片11"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8712.PNG" alt="幻灯片12"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8713.PNG" alt="幻灯片13"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8714.PNG" alt="幻灯片14"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8715.PNG" alt="幻灯片15"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8716.PNG" alt="幻灯片16"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8717.PNG" alt="幻灯片17"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8718.PNG" alt="幻灯片18"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8719.PNG" alt="幻灯片19"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8720.PNG" alt="幻灯片20"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8721.PNG" alt="幻灯片21"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8722.PNG" alt="幻灯片22"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8723.PNG" alt="幻灯片23"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8724.PNG" alt="幻灯片24"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8725.PNG" alt="幻灯片25"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8726.PNG" alt="幻灯片26"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8727.PNG" alt="幻灯片27"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8728.PNG" alt="幻灯片28"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8729.PNG" alt="幻灯片29"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8730.PNG" alt="幻灯片30"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8731.PNG" alt="幻灯片31"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8732.PNG" alt="幻灯片32"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8734.PNG"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8735.PNG" alt="幻灯片35"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8736.PNG" alt="幻灯片36"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8737.PNG" alt="幻灯片37"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8738.PNG" alt="幻灯片38"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8739.PNG" alt="幻灯片39"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8740.PNG" alt="幻灯片40"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/%E5%B9%BB%E7%81%AF%E7%89%8741.PNG" alt="幻灯片41"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/image-20220417105606361.png" alt="image-20220417105606361"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/image-20220417105621606.png" alt="image-20220417105621606"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/image-20220417105635386.png" alt="image-20220417105635386"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/image-20220417105651683.png" alt="image-20220417105651683"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/image-20220417105705448.png" alt="image-20220417105705448"></p><p><img src="/../images/ESPnet%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA-%E4%B8%80-%E6%80%BB%E8%A7%88/image-20220417105717563.png" alt="image-20220417105717563"></p>]]></content>
    
    
    <categories>
      
      <category>ESPnet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ESPnet</tag>
      
      <tag>End-to-End</tag>
      
      <tag>CTC/Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kaldi学习笔记（三）生成L.fst</title>
    <link href="/2022/04/14/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/"/>
    <url>/2022/04/14/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/</url>
    
    <content type="html"><![CDATA[<p>kaldi项目新路径：kaldi&#x2F;egs&#x2F;xuexi&#x2F;s5</p><p>L.fst是lexicon的WFST格式，L_disambig.fst引入了消歧符号。L.fst的输入音素序列，输出词序列。在这假设大家已经知道WFST,下面我们用一个小lexicon.txt来演示怎么生成L.fst等，词典放在data&#x2F;local&#x2F;dict下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small dict]$ cat lexicon.txt <br>SIL sil <br>&lt;SPOKEN_NOISE&gt; sil<br>你 n i3<br>的 d e5<br>原谅 vv van2 l iang4<br>也许 ii ie3 x v3<br>现在 x ian4 z ai4<br>如果 r u2 g uo3<br>认识 r en4 sh ix5<br>会 h ui4<br></code></pre></td></tr></table></figure><p>依靠这个字典我们准备以下文件：<br>nonsilence_phones.txt：语言直接相关的真实音素，同一行的音素是某一个音素的不同变体（重音、音调方面），故可共享决策树根，用的aishell的脚本；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small dict]$ cat lexicon.txt |awk &#x27;&#123; for(n=2;n&lt;=NF;n++)&#123; phones[$n] = 1; &#125;&#125; END&#123;for (p in phones) print p;&#125;&#x27;| perl -e &#x27;while(&lt;&gt;)&#123; chomp($_); $phone = $_; next if ($phone eq &quot;sil&quot;);<br>   m:^([^\d]+)(\d*)$: || die &quot;Bad phone $_&quot;; $q&#123;$1&#125; .= &quot;$phone &quot;; &#125;<br>   foreach $l (values %q) &#123;print &quot;$l\n&quot;;&#125;<br>&#x27;| sort -k1 &gt; nonsilence_phones.txt<br></code></pre></td></tr></table></figure><p>silence_phones.txt：静音类音素<br>optional_silence.txt：备用的静音类音素</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small dict]$ echo sil &gt; silence_phones.txt<br>(base) [lsy@small dict]$ echo sil &gt; optional_silence.txt<br></code></pre></td></tr></table></figure><p> extra_questions.txt：同一行的音素有着相同的重音或音调，与GMM训练中自动生成的“questions”一同用于决策树的生成。</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(16).png" alt="image (16)"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small dict]$ cat silence_phones.txt &gt; extra_questions.txt <br>(base) [lsy@small dict]$ cat nonsilence_phones.txt | perl -e &#x27;while(&lt;&gt;)&#123; foreach $p (split(&quot; &quot;, $_)) &#123;<br>  $p =~ m:^([^\d]+)(\d*)$: || die &quot;Bad phone $_&quot;; $q&#123;$2&#125; .= &quot;$p &quot;; &#125; &#125; foreach $l (values %q) &#123;print &quot;$l\n&quot;;&#125;&#x27;  &gt;&gt; extra_questions.txt<br></code></pre></td></tr></table></figure><p>这时候我们dict就准备好了，返回s5目录，下一步我们使用perpare_lang.sh来生成L.fst</p><p>首先连接wsj的steps和utils</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small s5]$ ln -s ~/kaldi/egs/wsj/s5/steps/ ./<br>(base) [lsy@small s5]$ ln -s ~/kaldi/egs/wsj/s5/utils/ ./<br></code></pre></td></tr></table></figure><p>然后使用perpare_lang.sh</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small s5]$ utils/prepare_lang.sh data/local/dict &quot;&lt;SPOKEN_NOISE&gt;&quot; data/local/lang data/lang<br></code></pre></td></tr></table></figure><p>这时候我们来看以下data&#x2F;lang文件夹下生成的东西</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(17).png" alt="image (17)"></p><p>我们可以通过fstprint和fstdraw进行可视化L.fst</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small lang]$ ~/kaldi/tools/openfst/bin/fstprint L.fst &gt; L_print.txt<br>(base) [lsy@small lang]$ ~/kaldi/tools/openfst/bin/fstprint --isymbols=phones.txt --osymbols=words.txt L.fst &gt; L_print1.txt<br></code></pre></td></tr></table></figure><p>一般我们用第二行命令进行可视化</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(18).png" alt="image (18)"></p><p>这个图的意思：</p><p>第16行只有一列，说明状态1是终止状态，并且没有权重。其余的都是5列，第一列是起点状态id，第二列是终点状态id，第三列是输入符号，第四列是输出符号，第五列是weight。因此第一行表示的边为：从状态0到1的边，输入是&lt;eps&gt;，输出是&lt;eps&gt;，权重为0.69314。这个WFST的初始状态是什么呢？OpenFst约定第一行的起点就是初始状态。</p><p>因此我们可以依靠此表将WFST画出来</p><p>现在再我们使用下面两行命令用fstdraw来进行可视化（因为这个词典太小可以可视化出来，一般这个命令没法用）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) [lsy@small lang]$ ~/kaldi/tools/openfst/bin/fstdraw --isymbols=phones.txt --osymbols=words.txt L.fst &gt; L.dot<br>(base) [lsy@small lang]$ dot -Tjpg L.dot &gt; L.jpg<br></code></pre></td></tr></table></figure><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/L%20(2)%20(1).jpg" alt="L (2) (1)"></p><p>我们现在来看一下lang文件夹下面的其他文件。</p><p>1.phones.txt，将所有音素一一映射为自然数，即音素 ID，引入“<eps>”（epsilon）、消歧（Disambiguation）符号“#n”（n 为自然数）， 便于 FST 处理。</p><p>第一列为音素，第二列为映射的自然素，可以看到总共122个音素，<eps>代表空，因为加了位置相关，B为音素在开头的意思，E为结尾，I为中间，S为单独一个，例如”SIL sil“就是单独的，”也许 ii ie3 x v3“ii就是开头的B， ie3和x就是中间I，v3就是结尾的E，后面都是要统计的。</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(19).png" alt="image (19)"></p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(20).png" alt="image (20)"></p><p>2.words.txt，将词一一映射为自然数，即词ID，引入“<eps>”（epsilon）、消歧符号 “#0”、“<s>”（句子起始处）、“</s>”（句子结尾处），便于 FST 处理；</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(21).png" alt="image (21)"></p><p>3.oov.txt，oov.int，集外词的替代者（此处为）及其在words.txt 中的ID；</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(22).png" alt="image (22)"></p><p>oov.txt就是如果出现界外词，就用oov.txt中的词代替。</p><p>4.topo，各个音素HMM模型的拓扑图，通过将一个音素（或三音素）表示成一个HMM，此文件确定了每个音素使用的HMM状态数以及转移概率，用于初始化单音素GMM-HMM，可根据需要自行进行修改（并用utils&#x2F;validate_lang.pl校验），实验中静音音素用了5个状态，其他音素用了3个状态；</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E7%94%9F%E6%88%90L-fst/image%20(23).png" alt="image (23)"></p><p>6-117是非静音音素，1-5为静音音素</p><p>5.phones&#x2F;，是dict&#x2F; 的拓展，内部文件均可以文本形式打开查看，后缀为 txt&#x2F;int&#x2F;csl 的同名文件之间是相互转换的，其中 context_indep.txt 标明了上下文无关建模的音素，通常为静音音素， wdisambig.txt&#x2F;wdisambig_phones.int&#x2F;wdisambig_words.int 分别标明了words.txt 引入的消歧符号（#0）及其在phones.txt 和words.txt 中的ID， roots.txt 定义了同一行音素的各个状态是否共享决策树的根以及是否拆分，对应的音素集则存放于sets.txt。</p><p>消歧是为了确保发音词典能够得到一个确定性的（Deterministic） WFST。 如果有些词对应的音素串是另一些词音素串的前缀，比如 good 的音素串是 goodness 的前半段音素串，需要在前者对应的音素串后面加入消歧音素，破坏这种前缀关系，这样， WFST 中一个词的路径就不会包含于另一个词的路径中。</p><p>未完待续。。。</p>]]></content>
    
    
    <categories>
      
      <category>kaldi</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kaldi</tag>
      
      <tag>L.fst</tag>
      
      <tag>WFST</tag>
      
      <tag>lexicon</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kaldi学习笔记（五）特征提取</title>
    <link href="/2022/04/14/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <url>/2022/04/14/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/</url>
    
    <content type="html"><![CDATA[<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>语音识别的第一步就是特征提取了，本文主要讲述如何根据音频信号提取MFCC和FBank特征（还有PLP，做孤立词识别用过，但没深究），这两种特征也是目前语音识别中使用最广泛的特征了。目前还有wav2vec也可以做特征提取，在无监督语音识别效果不错。</p><p>语音产生的过程：肺部呼出气体，然后通过声门的开启与闭合产生的周期信号，再通过声道产生声音，因为声道的不同，产生的声音也不同，比如拼音a、o、zi，三个声韵母，你会发现你的口型和牙齿的变化是不同的（可以看看<a href="http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/104S204/2">台大李琳山老师</a>的课）。而人类的语音信号大部分是在10000Hz以下，我们常使用的麦克风进行音频录制的采样率为16000Hz，一个采样点使用16bit来存储。</p><h3 id="MFCC特征提取步骤"><a href="#MFCC特征提取步骤" class="headerlink" title="MFCC特征提取步骤"></a>MFCC特征提取步骤</h3><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/2020_06_11_21_39_l6gHLo8NUjVt2iP.jpg" alt="2020_06_11_21_39_l6gHLo8NUjVt2iP"></p><h4 id="1-预加重"><a href="#1-预加重" class="headerlink" title="1.预加重"></a>1.预加重</h4><p>将语音信号通过一个预加重函数：</p><p>$$H(Z)&#x3D;1-\mu z^{-1}$$,式中u的值介于0.9-1.0之间，通常取0.97。预加重的目的是提升高频部分，使信号的频谱变得平坦，保持在低频到高频的整个频带中，能用同样的信噪比求频谱。这是因为很多声音在高频的地方会变得很微弱，通过这个方法，这只是实验性的方法，因为worker，所有大家都用。</p><h4 id="2-分帧"><a href="#2-分帧" class="headerlink" title="2.分帧"></a>2.分帧</h4><p>语音信号是一个非稳态、时变的信号，但是在极短的时间范围内，我们可以把语音认为是一个稳态、时不变的信号。这个极短时间范围通常分为20-40ms的帧，一般分成25ms为一帧。为了保证帧与帧之间平滑过渡，保持其连续性，分帧一般会让相邻的帧有重叠部分，因此每次只会移动10ms(而不是25ms)，这10ms我们称之为帧移。对于语音信号的采样频率是16kHz的，那么一帧就有16000*25&#x2F;1000&#x3D;400个样本点，帧移有16000 * 0.01 &#x3D; 160 个样本点，如果最后一帧不够400个样本点，我们一般在后面补0。对于一段语音如果有n个点，可以得到(n-400)&#x2F;160+1帧数据。</p><h4 id="3-加窗"><a href="#3-加窗" class="headerlink" title="3.加窗"></a>3.加窗</h4><p>首先一个基础：时域的乘积等于频域的卷积</p><p>因为后面要做FFT，我们不可能对所有时间做FFT，只能对短时长度的信号做FFT，因此我们只能在整段语音上截取一小段进行FFT。我们人为已经对这个无限长序列加了一个矩形窗，也就是这个无限长序列的频谱已经和一个矩形窗函数的频谱做了卷积了。这时候卷积可不是相乘，自然输出的这个有限长序列输出的频谱就变形了。</p><p>我们对一段信号进行分帧处理的时候，频谱泄漏会影响分析，所以才会用到窗函数，而经过窗函数处理的时域信号，其初始点和结束点的时域振幅都接近0。</p><p>在语音识别上主要用的是”汉明窗“。它能使信号在窗边界的值近似为 0，从而使得信号趋近于是一个周期信号，一个完整的有限长周期函数可以代表一个无限长周期函数，因此周期函数不会造成频谱泄露，该窗函数如下：$$w[n]&#x3D;0.54-0.46cos(\frac{2\pi n}{L}); 0\leq n \leq L-1$$，其他情况等于0。</p><h4 id="注意：以下为对每一个窗口进行的操作"><a href="#注意：以下为对每一个窗口进行的操作" class="headerlink" title="注意：以下为对每一个窗口进行的操作"></a>注意：以下为对每一个窗口进行的操作</h4><h4 id="4-离散傅里叶变换"><a href="#4-离散傅里叶变换" class="headerlink" title="4.离散傅里叶变换"></a>4.离散傅里叶变换</h4><p>DFT,将每个窗口内的数据从时域信号转为频域信号。DFT 的变换公式如下：</p><p>$$X(m)&#x3D;\sum_{n&#x3D;0}^{N-1}{x(n)h(n)e^{-j2\pi nm&#x2F;N}}$$</p><p>x(n) 是窗口中每个数据点的值，h(n)是一个数据点的窗函数，m是DFT的长度，e 是自然底数。有了X[m]我们就能估计功率谱：$$P_i(k)&#x3D;\frac{1}{N}|X_i(m)|^2$$</p><p>上式得到的是周期图的功率谱估计。通常我们会进行512点的DFT,并且因为对称性只保留前257（第一个点是直流分量）个系数。在实际中使用的一般是快速傅里叶变换（FFT，大家可以自行了解）。</p><h4 id="5-梅尔滤波器组"><a href="#5-梅尔滤波器组" class="headerlink" title="5.梅尔滤波器组"></a>5.梅尔滤波器组</h4><p>这个办法就是想要学人的耳朵怎么听语音的，研究人员发现听觉神经单元听到的是一堆频率而不是一个频率，而听觉神经是会听的重叠的，还有一个问题就是人耳对不同频率语音有不同的感知能力：对低频部分，与频率成线性关系；对高频部分，频率间隔越变越大。</p><p>因此研究人员想的一个办法就是用三角形滤波器来代替人耳神经单元，类似如下图所示：</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/20191023222025495.png" alt="20191023222025495"></p><p>这是一组大约20-40（通常26）个三角滤波器组，它会对上一步得到的周期图的功率谱估计进行滤波，每个滤波器组由26个(滤波器)长度为257的向量组成，每个滤波器的257个值中大部分都是0，只有对于需要采集的频率范围才是非零。输入的257点的信号会通过26个滤波器，我们会计算通过每个滤波器的信号的能量。</p><p>对于重叠部分的问题，解决办法是将两个三角形按上图所示叠放在一起，上一个滤波器的中间频率作为下一个滤波器的开始频率。</p><p>对于不同频率感知不同，就是将频率Hz转换成Mel频率来解决，转换公式如下：</p><p>$$m&#x3D;2595log_{10}(1+\frac{f}{700})$$</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/2020_06_11_21_40_mel.png" alt="2020_06_11_21_40_mel"></p><p>上图为Hz频率到Mel频率的转换。因此越到后面一个Mel滤波器对应的Hz频率越大，特性就是低频密，高频疏。</p><h4 id="6-能量取log"><a href="#6-能量取log" class="headerlink" title="6.能量取log"></a>6.能量取log</h4><p>字面意思，如果是40个滤波器，就对40个能量取log。也就得到了40维的Fbank特征。</p><p>为什么要取log:将在下一节介绍。</p><h4 id="7-IDFT"><a href="#7-IDFT" class="headerlink" title="7.IDFT"></a>7.IDFT</h4><p>FBank 特征的频谱图大概长下面这个样子，图中四个红点表示的是共振峰，是频谱图的主要频率，在语音识别中，根据共振峰来区分不同的音素（phone），所以我们可以把图中红线表示的特征提取出来就行，移除蓝色的影响部分。其中红色平滑曲线将各个共振峰连接起来，这条红线，称为谱包络（Spectral Envelope），蓝色上下震荡比较多的线条称为谱细节（Spectral details）。</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/2020_06_11_21_41_image-20200607174357596.png" alt="2020_06_11_21_41_image-20200607174357596"></p><p>首先，开头我们就讲了语音的产生可以理解为呼出的气通过声带振动E(w)，然后经过腔体（包括舌头、牙齿等等）H(w)，形成各种不同的发音X(w)。其中声带产生的频谱E(w)是很简单的，主要就是腔体决定着各个音素的频谱。所以如果我们知道腔体的信息，就可以准确的对音素进行描述。显然的，腔体的形状对应着上面图中的谱包络(红色的线)，揭示了共振峰的走向。</p><p>现在我们希望获得这个红色的线，但是这个红色的线被蓝色的线干扰的很厉害，我们要怎么把蓝色的线除掉，要怎么去掉呢：</p><p>在时间轴上我们的语音可以看作：$$X[w]&#x3D;E[w]*H[w]$$</p><p>将原来的语音经过傅里叶变换得到的频谱：$$X[w]&#x3D;E[w]H[w]$$</p><p>因为phase信息对语音识别来说没用，因此只考虑幅度就是：$$|X[w]|&#x3D;|E[w]||H[w]|$$</p><p>两边取log：$$log|X[w]|&#x3D;log|E[w]|+log|H[w]|$$</p><p>再在两边取逆傅里叶变换得到：$$X[w]&#x3D;E[w]+H[w]$$</p><p>log运算是为了分别包络和细节，包络代表音色，细节代表音高，显然语音识别就是为了识别音色。另外，人的感知与频率的对数成正比，正好使用log模拟，这就是取log的原因。</p><p>尽管现在E[w]+H[w]也就是包络和细节还是混在一起的，但是现在有一点不同，因为包络变化很慢，而细节变化很快，因此包络几乎在前面，而细节几乎再后面，所以我们如果在一个地方切开它，几乎就可以得到包络了，如下图所示。</p><p><img src="/../images/kaldi%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/image%20(15).png" alt="image (15)"></p><p>一般来说IDFT用的是DCT(离散余弦变换)，因为DCT具有去相关性，这时我们得到40维倒谱系数，最后我们保留2-13这个12维，这12维就叫MFCC特征。对功率谱再做DCT的目的就是为了提取信号的包络。</p><h4 id="8-Deltas和Delta-Deltas特征"><a href="#8-Deltas和Delta-Deltas特征" class="headerlink" title="8.Deltas和Delta-Deltas特征"></a>8.Deltas和Delta-Deltas特征</h4><p>Deltas和Delta-Deltas通常也叫(一阶)差分系数和二阶差分(加速度)系数。MFCC特征向量描述了一帧语音信号的功率谱的包络信息，但是语音识别也需要帧之间的动态变化信息，比如MFCC随时间的轨迹，实际证明把MFCC的轨迹变化加入后会提高识别的效果。因此我们可以用当前帧前后几帧的信息来计算Delta和Delta-Delta：</p><p>$$d_t&#x3D;\frac{\sum_{n&#x3D;1}^{N}{n(c_{t+n}-c_{t-n})}}{2\sum_{n&#x3D;1}^{N}{n^2}}$$</p><p>上式得到的dt是Delta系数，计算第t帧的Delta需要t-N到t+N的系数，N通常是2。如果对Delta系数dt再使用上述公式就可以得到Delta-Delta系数，这样我们就可以得到3*12&#x3D;36维的特征。上面也提到过，我们通常把能量也加到12维的特征里，对能量也可以计算一阶和二阶差分，这样最终可以得到39维的MFCC特征向量。</p><p>完成前面步骤后就是特征提取了，先看一下aishell脚本特征提取的代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><pre><code class="hljs plain">mfccdir=mfcc<br>for x in train dev test; do<br>  steps/make_mfcc_pitch.sh --cmd &quot;$train_cmd&quot; --nj 10 data/$x exp/make_mfcc/$x $mfccdir || exit 1;<br>  steps/compute_cmvn_stats.sh data/$x exp/make_mfcc/$x $mfccdir || exit 1;<br>  utils/fix_data_dir.sh data/$x || exit 1;<br>done<br></code></pre></td></tr></table></figure><p>未完待续。。。</p>]]></content>
    
    
    <categories>
      
      <category>kaldi</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kaldi</tag>
      
      <tag>mfcc</tag>
      
      <tag>特征提取</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于kaldi+GStreamer搭建web版实时语音识别系统</title>
    <link href="/2022/04/13/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/"/>
    <url>/2022/04/13/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<p>本文将会主要介绍怎么结合kaldi语音识别工具和两个GStreamer插件件(<a href="https://github.com/alumae/gst-kaldi-nnet2-online">gst­-kaldi­-nnet2­-online</a>、<a href="https://github.com/alumae/kaldi-gstreamer-server">kaldi­-gstreamer-server</a>)以及<a href="https://github.com/sendream/dictate.js">dictate.js</a>来搭建线上的实时语音识别系统。</p><p>本人配置环境：腾讯云服务器、ubuntu 18.04。</p><h2 id="一-kaldi"><a href="#一-kaldi" class="headerlink" title="一.kaldi"></a>一.kaldi</h2><p>Kaldi是当前最流行的开源语音识别工具(Toolkit)，它使用WFST来实现解码算法。Kaldi的主要代码是C++编写，在此之上使用bash和python脚本做了一些工具。而实时识别系统的好坏取决于语音识别的性能，语音识别包含特征提取、声学模型、语言模型、解码器等部分。Kaldi工具箱集成了几乎所有搭建语音识别器需要用到的工具。因不涉及GPU的使用，因此不用配置CUDA。kaldi的训练我将再另外一个专栏进行介绍。</p><h4 id="1-kaldi安装"><a href="#1-kaldi安装" class="headerlink" title="1.kaldi安装"></a><strong>1.kaldi安装</strong></h4><p>1.下载kaldi最新文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs plain">git clone https://github.com/kaldi-asr/kaldi.git<br></code></pre></td></tr></table></figure><p>2.安装kaldi，首先进入tools目录下，检查依赖性，没有的包按照指令安装，详细过程参见tools目录下的INSTALL文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd kaldi-trunk/tools<br>cat INSTALL<br>#查看安装教程<br>extras/check_dependencies.sh<br>#检查依赖性，没有的包按照指令安装<br>make  or  make -j 4(多线程加快进度）<br></code></pre></td></tr></table></figure><p>3.编译kaldi源文件，详细过程参见src目录下的INSTALL文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd ../src<br>cat INSTALL<br>#查看安装教程<br>./configure --shared<br>make depend -8<br>make -8<br></code></pre></td></tr></table></figure><p>安装过程遇上问题，需要停止安装并按照提示检查错误并解决错误，<a href="http://kaldi-asr.org/doc/build_setup.html">此处</a>为kaldi官方文档的编译过程。（我遇到的问题GCC版本太低）<br>4.检查是否安装成功</p><p>跑一个例子yesno</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd ../egs/yesno/s5<br>./run.sh<br></code></pre></td></tr></table></figure><p>输出显示<br><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(6).png" alt="image (6)"></p><p>表示安装成功</p><h4 id="2-模型的训练"><a href="#2-模型的训练" class="headerlink" title="2.模型的训练"></a>2.模型的训练</h4><p>模型训练将在kaldi专栏中进行介绍。本文将基于nnet3模型搭建实时语音识别系统，gst-kaldi-nnet-online插件需要的文件包括：final.mdl，frame_subsampling_factor，HCLG.fst，phones.txt，tree，words.txt；conf文件包括：ivector_extractor.conf，mfcc.conf，  online_cmvn.conf，online.conf，pitch.conf（如果使用音高），splice.conf；ivector_extractor文件包括：final.dubm，final.ie，final.mat，global_cmvn.stats，online_cmvn.conf，splice_opts。有了上面这些文件，我们就可以基于gst-kaldi-nnet-online插件搭建实时语音识别了，再结合kaldi­-gstreamer­-server和<a href="https://github.com/sendream/dictate.js">dictate.js</a>就可以实现web端语音识别系统。</p><h2 id="二-GStreamer插件"><a href="#二-GStreamer插件" class="headerlink" title="二.GStreamer插件"></a>二.GStreamer插件</h2><h4 id="1-安装gst-kaldi-nnet-online插件"><a href="#1-安装gst-kaldi-nnet-online插件" class="headerlink" title="1.安装gst-kaldi-nnet-online插件"></a>1.安装gst-kaldi-nnet-online插件</h4><p>首先安装gstreamer-1.0：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">sudo apt-get install gstreamer1.0-plugins-bad  gstreamer1.0-plugins-base gstreamer1.0-plugins-good  gstreamer1.0-pulseaudio  gstreamer1.0-plugins-ugly  gstreamer1.0-tools libgstreamer1.0-dev<br></code></pre></td></tr></table></figure><p>Ubuntu14.04版本以下，在执行上述sudo apt-get install之前，需要使用backport ppa来获取gstreamer的1.0版本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">sudo add-apt-repository ppa:gstreamer-developers/ppa<br>sudo apt-get update<br></code></pre></td></tr></table></figure><p>接下来就是安装 Jansson 库开发包（2.7 或更高版本），用于将结果编码为 JSON：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">sudo apt-get install libjansson-dev<br></code></pre></td></tr></table></figure><p>接下来就是编译安装gst-kaldi-nnet-online插件了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plain">git clone https://github.com/alumae/gst­kaldi­nnet2­online.git<br>#如果无法下载就自己去github下载上传到服务器<br>cd gst-kaldi-nnet2-online/src<br>KALDI_ROOT=/path/of/kaldi-trunk make depend<br>KALDI_ROOT=/path/of/kaldi-trunk make<br>#编译指定kaldi的根目录<br></code></pre></td></tr></table></figure><p>整个编译如果没有出现错误，那么应该会在src目录下产生libgstkaldinnet2onlinedecoder.so<br><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(7).png" alt="image (7)"></p><p>设置环境变量</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">vim ~/.bashrc<br>#按GG到最后一行插入下面这行命令<br>export GST_PLUGIN_PATH=your gst­kaldi­nnet2­online installation directory/src<br></code></pre></td></tr></table></figure><p>测试GStreamer 是否可以访问插件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">GST_PLUGIN_PATH=. gst-inspect-1.0 kaldinnet2onlinedecoder<br></code></pre></td></tr></table></figure><p>输出应列出所有插件属性及其默认值：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs plain">Factory Details:<br>  Rank                     none (0)<br>  Long-name                KaldiNNet2OnlineDecoder<br>  Klass                    Speech/Audio<br>  Description              Convert speech to text<br>[...]<br>  name                : The name of the object<br>                        flags: readable, writable<br>                        String. Default: &quot;kaldinnet2onlinedecoder0&quot;<br>  parent              : The parent of the object<br>                        flags: readable, writable<br>                        Object of type &quot;GstObject&quot;<br>  silent              : Silence the decoder<br>                        flags: readable, writable<br>                        Boolean. Default: false<br>  model               : Filename of the acoustic model<br>                        flags: readable, writable<br>                        String. Default: &quot;models/final.mdl&quot;<br>[...]<br>  max-nnet-batch-size : Maximum batch size we use in neural-network decodable object, in cases where we are not constrained by currently available frames (this will rarely make a difference)<br>                        flags: readable, writable<br>                        Integer. Range: -2147483648 - 2147483647 Default: 256<br><br>Element Signals:<br>  &quot;partial-result&quot; :  void user_function (GstElement* object,<br>                                          gchararray arg0,<br>                                          gpointer user_data);<br>  &quot;final-result&quot; :  void user_function (GstElement* object,<br>                                        gchararray arg0,<br>                                        gpointer user_data);<br></code></pre></td></tr></table></figure><h4 id="2-测试语音识别系统"><a href="#2-测试语音识别系统" class="headerlink" title="2.测试语音识别系统"></a>2.测试语音识别系统</h4><p>安装完gst-kaldi-nnet-online插件后，配合kaldi语音识别工具箱，就可以实现实时语音识别了，在demo提供了两个案例，下面将详细介绍这两个案例。</p><p>1.案例一</p><p>首先要下载基于DNN的英语模型。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd demo<br>./prepare-models.sh<br></code></pre></td></tr></table></figure><p>将会下载三个文件夹models、conf、ivector_extractor：<br>![image (8)](..&#x2F;images&#x2F;基于kaldi-GStreamer搭建web版实时语音识别系统&#x2F;image (8).png)</p><p><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/IMG_20220404_105214020.jpg" alt="IMG_20220404_105214020"></p><p><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(9).png" alt="image (9)"></p><p>有了以上文件后，直接运行transcribe­audio.sh这个脚本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">./transcribe-audio.sh dr_strangelove.mp3<br></code></pre></td></tr></table></figure><p>得到以下结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs plain">LOG ([5.5.201~1-36f6d]:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor<br>LOG ([5.5.201~1-36f6d]:ComputeDerivedVars():ivector-extractor.cc:204) Done.<br>huh i hello this is hello dimitri listen i i can&#x27;t hear too well do you support you could turn the music down just a little<br>ha ha that&#x27;s much better yet not yet<br>...<br></code></pre></td></tr></table></figure><p>查看transcribe-audio.sh可以看出怎么配置gst-kaldi-nnet2-online,因为后面­kaldi-gstreamer-­server会涉及到，以下是该脚本核心配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs plain">GST_PLUGIN_PATH=../src gst-launch-1.0 --gst-debug=&quot;&quot; -q filesrc location=$audio ! decodebin ! audioconvert ! audioresample ! \<br>kaldinnet2onlinedecoder \<br>  use-threaded-decoder=true \<br>  model=models/final.mdl \<br>  fst=models/HCLG.fst \<br>  word-syms=models/words.txt \<br>  phone-syms=models/phones.txt \<br>  word-boundary-file=models/word_boundary.int \<br>  num-nbest=3 \<br>  num-phone-alignment=3 \<br>  do-phone-alignment=true \<br>  feature-type=mfcc \<br>  mfcc-config=conf/mfcc.conf \<br>  ivector-extraction-config=conf/ivector_extractor.fixed.conf \<br>  max-active=7000 \<br>  beam=11.0 \<br>  lattice-beam=5.0 \<br>  do-endpointing=true \<br>  endpoint-silence-phones=&quot;1:2:3:4:5:6:7:8:9:10&quot; \<br>  chunk-length-in-secs=0.2 \<br>! filesink location=/dev/stdout buffer-mode=2<br></code></pre></td></tr></table></figure><p>小声哔哔1：在运行这步的时候出错了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">INTEL MKL ERROR: /opt/intel/mkl/lib/intel64/libmkl_avx2.so: undefined symbol: mkl_sparse_optimize_bsr_trsm_i8.<br>Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.<br></code></pre></td></tr></table></figure><p>出现上述错误貌似是MKL没有安装好，但是kaldi我能正常运行，我经过下述操作解决了该问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd ~/.bashrc<br>#(GG)跳到最后一行<br>export LD_PRELOAD=/opt/intel/mkl/lib/intel64/libmkl_def.so:/opt/intel/mkl/lib/intel64/libmkl_avx2.so:/opt/intel/mkl/lib/intel64/libmkl_core.so:/opt/intel/mkl/lib/intel64/libmkl_intel_lp64.so:/opt/intel/mkl/lib/intel64/libmkl_intel_thread.so:/opt/intel/lib/intel64_lin/libiomp5.so:/opt/intel/mkl/lib/intel64/libmkl_sequential.so<br>#本来只要添加两个，但是后面还有同类错误，所有全部添加了<br></code></pre></td></tr></table></figure><p>小声哔哔2：解决第一个问题后又出现问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs plain">(base) ubuntu@VM-4-17-ubuntu:~/gst-kaldi-nnet2-online/demo$ ./transcribe-audio.sh dr_strangelove.mp3<br>LOG ([5.5.201~1-36f6d]:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor<br>LOG ([5.5.201~1-36f6d]:ComputeDerivedVars():ivector-extractor.cc:204) Done.<br>ERROR: from element /GstPipeline:pipeline0/GstDecodeBin:decodebin0: Your GStreamer installation is missing a plug-in.<br>Additional debug info:<br>gstdecodebin2.c(4640): gst_decode_bin_expose (): /GstPipeline:pipeline0/GstDecodeBin:decodebin0:<br>no suitable plugins found:<br>Missing decoder: ID3 tag (application/x-id3)<br><br>ERROR: pipeline doesn&#x27;t want to preroll.<br></code></pre></td></tr></table></figure><p>当时查了很多资料无法解决该问题，因为装了anaconda3，当时是在base环境下，后面conda deactivate该问题消失。<br>2.案例二</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">python2 gui-demo.py<br></code></pre></td></tr></table></figure><p>运行这个基本需要用python2，运行完成后将会弹出一个小框框，点击Speak按钮后开始实时语音识别。<br>在运行这个引入包的时候出错了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">from gi.repository import GObject, Gst, Gtk, Gdk<br></code></pre></td></tr></table></figure><p>解决方法忘了，自己解决吧。<br>以下是对gui-demo.py程序代码，注解参考<a href="https://github.com/mrjunjieli">李健</a>的热心分享</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><code class="hljs plain">#!/usr/bin/env python<br># <br># Copyright (c) 2013 Tanel Alumae<br># Copyright (c) 2008 Carnegie Mellon University.<br>#<br># Inspired by the CMU Sphinx&#x27;s Pocketsphinx Gstreamer plugin demo (which has BSD license)<br>#<br># Licence: BSD<br><br>import sys<br>import os<br>import gi<br>gi.require_version(&#x27;Gst&#x27;, &#x27;1.0&#x27;)#<br>from gi.repository import GObject, Gst, Gtk, Gdk<br>GObject.threads_init()<br>Gdk.threads_init()<br><br>Gst.init(None)<br><br>class DemoApp(object):<br>    &quot;&quot;&quot;GStreamer/Kaldi Demo Application&quot;&quot;&quot;<br>    def __init__(self):<br>        &quot;&quot;&quot;Initialize a DemoApp object&quot;&quot;&quot;<br>        self.init_gui()<br>    def init_gui(self):<br>        &quot;&quot;&quot;Initialize the GUI components&quot;&quot;&quot;<br>        self.window = Gtk.Window()<br>        self.window.set_border_width(10)<br>        vbox = Gtk.VBox()<br>        self.text = Gtk.TextView()<br>        self.textbuf = self.text.get_buffer()<br>        self.text.set_wrap_mode(Gtk.WrapMode.WORD)<br>        vbox.pack_start(self.text, True, True, 1)<br>        self.button = Gtk.Button(&quot;Speak&quot;)<br>        self.button.connect(&#x27;clicked&#x27;, self.button_clicked)<br>        vbox.pack_start(self.button, False, False, 5)<br>        self.window.add(vbox)<br>        self.window.show_all()<br><br>    def quit(self, window):<br>        Gtk.main_quit()<br><br>    def init_gst(self):<br>        &quot;&quot;&quot;Initialize the speech components&quot;&quot;&quot;<br>        self.pulsesrc = Gst.ElementFactory.make(&quot;pulsesrc&quot;, &quot;pulsesrc&quot;)<br>        if self.pulsesrc == None:<br>            sys.exit()<br>        self.audioconvert = Gst.ElementFactory.make(&quot;audioconvert&quot;, &quot;audioconvert&quot;)<br>        self.audioresample = Gst.ElementFactory.make(&quot;audioresample&quot;, &quot;audioresample&quot;)<br>        self.asr = Gst.ElementFactory.make(&quot;kaldinnet2onlinedecoder&quot;, &quot;asr&quot;)<br>        self.fakesink = Gst.ElementFactory.make(&quot;fakesink&quot;, &quot;fakesink&quot;)<br><br>        if self.asr:<br>          model_file = &quot;models/final.mdl&quot;<br>          if not os.path.isfile(model_file):<br>              print &gt;&gt; sys.stderr, &quot;Models not downloaded? Run prepare-models.sh first!&quot;<br>              sys.exit(1)<br>          self.asr.set_property(&quot;nnet-mode&quot;,3)<br>          self.asr.set_property(&quot;fst&quot;, &quot;models/HCLG.fst&quot;)<br>          self.asr.set_property(&quot;model&quot;, model_file)<br>          self.asr.set_property(&quot;word-syms&quot;, &quot;models/words.txt&quot;)<br>          self.asr.set_property(&quot;feature-type&quot;, &quot;mfcc&quot;)<br>          self.asr.set_property(&quot;mfcc-config&quot;, &quot;conf/mfcc.conf&quot;)<br>          self.asr.set_property(&quot;ivector-extraction-config&quot;, &quot;conf/ivector_extractor.fixed.conf&quot;)<br>          self.asr.set_property(&quot;max-active&quot;, 7000)<br>          self.asr.set_property(&quot;beam&quot;, 10.0)<br>          self.asr.set_property(&quot;lattice-beam&quot;, 6.0)<br>          self.asr.set_property(&quot;do-endpointing&quot;, True)<br>          self.asr.set_property(&quot;endpoint-silence-phones&quot;, &quot;1:2:3:4:5:6:7:8:9:10&quot;)<br>          self.asr.set_property(&quot;use-threaded-decoder&quot;, False)<br>          self.asr.set_property(&quot;chunk-length-in-secs&quot;, 0.2)<br>        else:<br>          print &gt;&gt; sys.stderr, &quot;Couldn&#x27;t create the kaldinnet2onlinedecoder element. &quot;<br>          if os.environ.has_key(&quot;GST_PLUGIN_PATH&quot;):<br>            print &gt;&gt; sys.stderr, &quot;Have you compiled the Kaldi GStreamer plugin?&quot;<br>          else:<br>            print &gt;&gt; sys.stderr, &quot;You probably need to set the GST_PLUGIN_PATH envoronment variable&quot;<br>            print &gt;&gt; sys.stderr, &quot;Try running: GST_PLUGIN_PATH=../src %s&quot; % sys.argv[0]<br>          sys.exit();<br><br>        # initially silence the decoder<br>        self.asr.set_property(&quot;silent&quot;, True)<br><br>        self.pipeline = Gst.Pipeline()<br>        for element in [self.pulsesrc, self.audioconvert, self.audioresample, self.asr, self.fakesink]:<br>            self.pipeline.add(element)<br>        self.pulsesrc.link(self.audioconvert)<br>        self.audioconvert.link(self.audioresample)<br>        self.audioresample.link(self.asr)<br>        self.asr.link(self.fakesink)<br><br>        self.asr.connect(&#x27;partial-result&#x27;, self._on_partial_result)<br>        self.asr.connect(&#x27;final-result&#x27;, self._on_final_result)<br>        self.pipeline.set_state(Gst.State.PLAYING)<br><br><br><br>    def _on_partial_result(self, asr, hyp):<br>        &quot;&quot;&quot;Delete any previous selection, insert text and select it.&quot;&quot;&quot;<br>        Gdk.threads_enter()<br>        # All this stuff appears as one single action<br>        self.textbuf.begin_user_action()<br>        self.textbuf.delete_selection(True, self.text.get_editable())<br>        self.textbuf.insert_at_cursor(hyp)<br>        ins = self.textbuf.get_insert()<br>        iter = self.textbuf.get_iter_at_mark(ins)<br>        iter.backward_chars(len(hyp))<br>        self.textbuf.move_mark(ins, iter)<br>        self.textbuf.end_user_action()<br>        Gdk.threads_leave()<br><br>    def _on_final_result(self, asr, hyp):<br>        Gdk.threads_enter()<br>        &quot;&quot;&quot;Insert the final result.&quot;&quot;&quot;<br>        # All this stuff appears as one single action<br>        self.textbuf.begin_user_action()<br>        self.textbuf.delete_selection(True, self.text.get_editable())<br>        self.textbuf.insert_at_cursor(hyp)<br>        if (len(hyp) &gt; 0):<br>            self.textbuf.insert_at_cursor(&quot; &quot;)<br>        self.textbuf.end_user_action()<br>        Gdk.threads_leave()<br><br><br><br>    def button_clicked(self, button):<br>        &quot;&quot;&quot;Handle button presses.&quot;&quot;&quot;<br>        if button.get_label() == &quot;Speak&quot;:<br>            button.set_label(&quot;Stop&quot;)<br>            self.asr.set_property(&quot;silent&quot;, False)<br>        else:<br>            button.set_label(&quot;Speak&quot;)<br>            self.asr.set_property(&quot;silent&quot;, True)<br><br><br><br>if __name__ == &#x27;__main__&#x27;:<br>  app = DemoApp()<br>  Gdk.threads_enter()<br>  Gtk.main()<br>  Gdk.threads_leave()<br><br></code></pre></td></tr></table></figure><h2 id="三-Kaldi­-gstreamer­-server插件"><a href="#三-Kaldi­-gstreamer­-server插件" class="headerlink" title="三.Kaldi­-gstreamer­-server插件"></a>三.Kaldi­-gstreamer­-server插件</h2><p>该插件下所有python脚本需用python2运行</p><h4 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1.安装依赖"></a>1.安装依赖</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plain">#Tornado 4, 见 http://www.tornadoweb.org/en/stable/<br>#ws4py (0.3.0 .. 0.3.2)<br>#YAML<br>#JSON<br>sudo pip install tornado<br>sudo pip install ws4py==0.3.2<br>sudo pip install pyyaml<br></code></pre></td></tr></table></figure><p>测试是否满足所有依赖关系<br><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(10).png" alt="image (10)"></p><p>没有出现no module错误表示安装正确，出错缺啥装啥</p><h4 id="2-安装Kaldi­-gstreamer­-server插件"><a href="#2-安装Kaldi­-gstreamer­-server插件" class="headerlink" title="2.安装Kaldi­-gstreamer­-server插件"></a>2.安装Kaldi­-gstreamer­-server插件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd ~<br>#我安装在家目录下<br>git clone https://github.com/alumae/kaldi­gstreamer­server.git <br></code></pre></td></tr></table></figure><p>下载完成后即可使用。<br>kaldi­-gstreamer­-server&#x2F;kaldigstserver下存放的是核心程序。整个server包含两部分，第一个是master_server.py，master_server不进行语音识别，它的作用是接收和发送数据。第二个是worker.py，worker的作用是对接收的进行语音识别并发送识别结果。使用的是websocket全双工通信。因此识别流程是“客户端”发送数据到master_server，master_server将识别任务分配给worker(当有多个客户端请求时master_server可以把不同的任务分配给不同的worker)，worker接收数据识别后将识别结果传回master_server，master_server再将识别结果返回给客户端。</p><p>下面展示如何对实现识别语音识别：</p><h4 id="3-运行服务器"><a href="#3-运行服务器" class="headerlink" title="3.运行服务器"></a>3.运行服务器</h4><p>首先下载训练好的nnet2模型，用作测试使用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd kaldi-gstreamer-server-master/test/models/<br>./download-multi_cn-nnet3.sh<br>cd ../..<br></code></pre></td></tr></table></figure><p>运行master server，端口号为8888（可以自己随意设置）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">python2 kaldigstserver/master_server.py --port=8888<br></code></pre></td></tr></table></figure><p>接下来开启worker,worker负责语音识别部分，worker可以使用两种解码器</p><p>第一种：onlinegmmdecodefasterGStreamer，支持GMM,安装教程如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd kaldi/src<br>make ext<br>cd tools<br>./install_portaudio.sh<br>vim ~/.bashrc<br>加入path/kaldi/tools/portaudio<br>sudo ldconfig<br>cd src/gst-plugin/<br>KALDI_ROOT=/path/of/kaldi make depend<br>KALDI_ROOT=/path/of/kaldi make<br>然后会在src/gst-plugin中看到libgstonlinegmmdecodefaster.so<br>export GST_PLUGIN_PATH=pathkaldi/src/gst-plugin (可以把这个目录写入~/.bashrc中)<br>gst-inspect-1.0 onlinegmmdecodefaster<br></code></pre></td></tr></table></figure><p>我们使用的是第二种较新的kaldinnet2onlinedecoder插件，支持DNN模型</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">python2 kaldigstserver/worker.py -u ws://localhost:8888/worker/ws/speech -c /home/ubuntu/kaldi-gstreamer-server-master/sample_chinese_nnet3.yam<br></code></pre></td></tr></table></figure><p>该-u ws:&#x2F;&#x2F;localhost:8888&#x2F;worker&#x2F;ws&#x2F;speech参数指定worker应连接到的master server所在的ip地址（本机为localhost）。并且确保worker使用的端口号与master server相同的端口（此处为8888），你可以同时启动任意数量的worker,只需要将上面命令再运行就可以了。<br>启动master server和woeker后就是客户端的使用了，客户端的示例kaldigstserver&#x2F;client.py</p><p>，可以通过调用下面的命令测试安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">python2 kaldigstserver/client.py -r 8192 test/data/chinese_test.wav<br></code></pre></td></tr></table></figure><p>没有问题的话应该会出现识别结果如下<br><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(11).png" alt="image (11)"></p><p>但是我当时出现问题，运行的时候出现错误并返回state1。</p><p>当时是将worker.py中422行代码修改成423行的样子解决的，刚想复现这个问题发现又不报错了，神奇。</p><p><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(12).png" alt="image (12)"></p><h2 id="四-搭建web端实时语音识别系统"><a href="#四-搭建web端实时语音识别系统" class="headerlink" title="四.搭建web端实时语音识别系统"></a>四.搭建web端实时语音识别系统</h2><p>当上述步骤完成后，我们将借助<a href="https://github.com/sendream/dictate.js">dictate.js</a>搭建web端实时语音识别系统。</p><h4 id="1-安装nginx"><a href="#1-安装nginx" class="headerlink" title="1.安装nginx"></a>1.安装nginx</h4><p>apt-get安装nginx</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">sudo apt-get install nginx<br></code></pre></td></tr></table></figure><p>查看nginx是否安装成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">nginx -V<br></code></pre></td></tr></table></figure><p>启动nginx</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">service nginx start <br></code></pre></td></tr></table></figure><p>启动后，在浏览器输入ip地址，可以看到nginx的欢迎页面，表示nginx安装成功</p><h4 id="2-申请域名"><a href="#2-申请域名" class="headerlink" title="2.申请域名"></a>2.申请域名</h4><p> 因为dictate.js调用麦克风需要https传输，而使用https传输不能使用ws，而是要使用wss（开始用apache配置wss差点人都去世了，后面转用nginx），配置wss需要域名(网上这么说的)。</p><p>打开xx云，搜索域名，然后购买9块钱一年。因为要备案，我用了个备了案的二级域名。</p><p>申请完域名后绑定服务器ip地址</p><h4 id="3-申请并配置SSL"><a href="#3-申请并配置SSL" class="headerlink" title="3.申请并配置SSL"></a>3.申请并配置SSL</h4><p>打开腾讯云(别的也一样)，搜索ssl,点击“立即选购”，点击”自定义配置“，选择域名型免费版：</p><p><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(13).png" alt="image (13)"></p><p>然后按要求填就是了。</p><p>签发证书后在我的证书下载nginx版，下载完成后上传到服务器。</p><p>后续步骤参考<a href="https://cloud.tencent.com/developer/article/1160294">这里</a></p><h4 id="4-部署dictate-js"><a href="#4-部署dictate-js" class="headerlink" title="4.部署dictate.js"></a>4.部署dictate.js</h4><p><strong>假设你的域名为kfc.zym</strong></p><p>上述步骤完成后，应该可以在浏览器使用<a href="https://kfc.zym域/">https://kfc.zy</a><a href="https://kfc.zym/">m</a>访问了</p><p>将dictate.js解压放在&#x2F;var&#x2F;www&#x2F;html&#x2F;文件夹下</p><p>修改&#x2F;etc&#x2F;nginx&#x2F;sites-enabled&#x2F;default</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs plain">sudo vim /etc/nginx/sites-enabled/default<br>找到loaction /&#123;&#125;修改成:<br>location / &#123;<br>  root /var/www/html;<br>  index  index.html index.htm /web/;<br>&#125;<br></code></pre></td></tr></table></figure><p>此时通过<a href="https://kfc.zym/web/demos/mob">https://kfc.zym/web/demos/mob</a><a href="https://kfc.zym/web/demos/demo.html">.html</a><br>为了让wss能够传输，应在上述location&#x2F;{}后面添加</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs plain">location /api/ &#123;# /api/为你代理转换的字符串<br>           proxy_pass http://127.0.0.1:8888/; <br>           #127.0.0.1为master server的ip地址，8888为master server设置的端口号<br>           proxy_http_version 1.1;<br>           proxy_set_header Upgrade $http_upgrade;<br>           proxy_set_header Connection &quot;upgrade&quot;;<br>    &#125;<br></code></pre></td></tr></table></figure><p>这时应该修改mob.html中的wss</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">&lt;option value=&quot;wss://abc.zym:443/api/client/ws/speech|wss://abc.zym:443/api/client/ws/status&quot; selected=&quot;selected&quot;&gt;普通话&lt;/option&gt;<br>#abc.zym为你的域名，端口号为443,不能设为8888，/api/为上一步设置的跳转，nginx中是什么这里就是什么<br></code></pre></td></tr></table></figure><h4 id="5-测试web端语音识别系统是否能用"><a href="#5-测试web端语音识别系统是否能用" class="headerlink" title="5.测试web端语音识别系统是否能用"></a>5.测试web端语音识别系统是否能用</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs plain">sudo service nginx restart #重启nginx<br>cd ~/kaldi-gstreamer-server-master/<br>python2 kaldigstserver/master_server.py --port=8888<br>python2 kaldigstserver/worker.py -u ws://localhost:8888/worker/ws/speech -c /home/ubuntu/kaldi-gstreamer-server-master/sample_chinese_nnet3.yaml<br></code></pre></td></tr></table></figure><p>在浏览器进入<a href="https://kfc.zym/web/demos/mob.html">https://kfc.zym/web/demos/mob.html</a><br>进行测试，如果能实时识别就说明成功了</p><h4 id="6-自己训练的模型部署"><a href="#6-自己训练的模型部署" class="headerlink" title="6.自己训练的模型部署"></a>6.自己训练的模型部署</h4><p>参考下面文件夹所需配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">kaldi-gstreamer-server-master/test/models/chinese/multi_cn_chain_sp_online<br></code></pre></td></tr></table></figure><p>仿照下面yaml填写自己的yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">kaldi-gstreamer-server-master/sample_chinese_nnet3.yaml<br></code></pre></td></tr></table></figure><p>如果只是使用了的mfcc，则没有大的变化<br>但是如果使用了音高则需要在yaml中添加pitch</p><p>以下为我的yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs plain"># You have to download multi_cn &quot;online nnet3&quot; chain model in order to use this sample<br># Run download-multi_cn-nnet3.sh in &#x27;test/models&#x27; to download them.<br>use-nnet2: True<br>decoder:<br>    # All the properties nested here correspond to the kaldinnet2onlinedecoder GStreamer plugin properties.<br>    # Use gst-inspect-1.0 ./libgstkaldionline2.so kaldinnet2onlinedecoder to discover the available properties<br>    nnet-mode : 3<br>    use-threaded-decoder:  true<br>    add-pitch: true<br>    model : /home/ubuntu/tibetan_lasha/final.mdl<br>    word-syms : /home/ubuntu/tibetan_lasha/words.txt<br>    fst : /home/ubuntu/tibetan_lasha/HCLG.fst<br>    mfcc-config : /home/ubuntu/tibetan_lasha/conf/mfcc.conf<br>    online-pitch-config: /home/ubuntu/tibetan_lasha/conf/pitch.conf<br>    ivector-extraction-config : /home/ubuntu/tibetan_lasha/conf/ivector_extractor.conf<br>    max-active: 7000<br>    beam: 15.0<br>    lattice-beam: 8.0<br>    acoustic-scale: 0.1<br>    do-endpointing : true<br>    endpoint-silence-phones : &quot;1:2:3:4:5:6:7:8:9:10:11:12:13:14:15&quot;<br>    traceback-period-in-secs: 0.25<br>    chunk-length-in-secs: 0.25<br>    num-nbest: 1<br>out-dir: tmp<br>use-vad: False<br>silence-timeout: 10<br><br># Just a sample post-processor that appends &quot;.&quot; to the hypothesis<br>post-processor: perl -npe &#x27;BEGIN &#123;use IO::Handle; STDOUT-&gt;autoflush(1);&#125; sleep(1); s/(.*)/\1./;&#x27;<br><br>#post-processor: (while read LINE; do echo $LINE; done)<br><br># A sample full post processor that add a confidence score to 1-best hyp and deletes other n-best hyps<br>#full-post-processor: ./sample_full_post_processor.py<br><br>logging:<br>    version : 1<br>    disable_existing_loggers: False<br>    formatters:<br>        simpleFormater:<br>            format: &#x27;%(asctime)s - %(levelname)7s: %(name)10s: %(message)s&#x27;<br>            datefmt: &#x27;%Y-%m-%d %H:%M:%S&#x27;<br>    handlers:<br>        console:<br>            class: logging.StreamHandler<br>            formatter: simpleFormater<br>            level: DEBUG<br>    root:<br>        level: DEBUG<br>        handlers: [console]<br></code></pre></td></tr></table></figure><h2 id="五-展示效果"><a href="#五-展示效果" class="headerlink" title="五.展示效果"></a>五.展示效果</h2><p>已经实现效果展示，可点击<a href="https://csl.cqspclsm.com/nmu/csl/mob_asr.html">这里</a>进行测试</p><p><img src="/../images/%E5%9F%BA%E4%BA%8Ekaldi-GStreamer%E6%90%AD%E5%BB%BAweb%E7%89%88%E5%AE%9E%E6%97%B6%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/image%20(14).png" alt="image (14)"></p><p>感谢“克维斯利姆·德里奥”和”奥雷里亚诺上校”的插图</p>]]></content>
    
    
    <categories>
      
      <category>kaldi</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kaldi</tag>
      
      <tag>GStreamer</tag>
      
      <tag>语音识别系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ESPnet安装</title>
    <link href="/2022/04/13/ESPnet%E5%AE%89%E8%A3%85/"/>
    <url>/2022/04/13/ESPnet%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="版本：cuda-10-1-pytorch1-4-centos7-8"><a href="#版本：cuda-10-1-pytorch1-4-centos7-8" class="headerlink" title="版本：cuda 10.1 pytorch1.4  centos7.8"></a>版本：cuda 10.1 pytorch1.4  centos7.8</h1><h1 id="0-准备"><a href="#0-准备" class="headerlink" title="0.准备"></a>0.准备</h1><p>需要准备安装好anaconda3，kalid(可以见kaldi安装)</p><h1 id="1-创建虚拟环境并激活"><a href="#1-创建虚拟环境并激活" class="headerlink" title="1.创建虚拟环境并激活"></a>1.创建虚拟环境并激活</h1><p>执行下面命令创建虚拟环境：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><pre><code class="hljs plain">conda create -n espnet python=3.7.3<br>conda activate espnet<br></code></pre></td></tr></table></figure><p>conda换源，建议换清华的源，阿里的源下载的pytorch和cuda以及cudnn的版本和清华的不同，取决于你的cuda_cudnn的版本，不然会报错<br>pip换源</p><p>参考：<a href="https://blog.csdn.net/zhayushui/article/details/80433768">https://blog.csdn.net/zhayushui/article/details/80433768</a></p><h1 id="2-安装pytorch"><a href="#2-安装pytorch" class="headerlink" title="2.安装pytorch"></a>2.安装pytorch</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">conda install pytorch==1.4.0 torchvision==0.5.0 cudatoolkit=10.1<br></code></pre></td></tr></table></figure><p>上面为例子，需要自己去pytorch找对应版本。pytorch官网：<a href="https://pytorch.org/">https://pytorch.org/</a></p><h1 id="3-克隆espnet"><a href="#3-克隆espnet" class="headerlink" title="3.克隆espnet"></a>3.克隆espnet</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd ~<br>git clone https://github.com/espnet/espnet<br></code></pre></td></tr></table></figure><p>我安装在家目录下，自己视情况决定安装的位置<br>可能github网速挺慢的导致克隆失败，也需要换国内github源</p><h1 id="4-在espnet-x2F-tools下面链接kaldi"><a href="#4-在espnet-x2F-tools下面链接kaldi" class="headerlink" title="4.在espnet&#x2F;tools下面链接kaldi"></a>4.在espnet&#x2F;tools下面链接kaldi</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd espnet/tools<br>ln -s ~/kaldi ./<br></code></pre></td></tr></table></figure><h1 id="5-设置CUDA环境"><a href="#5-设置CUDA环境" class="headerlink" title="5.设置CUDA环境"></a>5.设置CUDA环境</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">. ./setup_cuda_env.sh ~/cuda-10.1<br></code></pre></td></tr></table></figure><p>注意上面的cuda环境是我自己的环境</p><h1 id="6-设置系统python环境"><a href="#6-设置系统python环境" class="headerlink" title="6.设置系统python环境"></a>6.设置系统python环境</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">./setup_python.sh $(command -v python3)<br></code></pre></td></tr></table></figure><h1 id="7-安装"><a href="#7-安装" class="headerlink" title="7.安装"></a>7.安装</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">make<br></code></pre></td></tr></table></figure><h1 id="8-安装warp-ctc"><a href="#8-安装warp-ctc" class="headerlink" title="8.安装warp-ctc"></a>8.安装warp-ctc</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd installers<br>./install_warp-ctc.sh<br></code></pre></td></tr></table></figure><h1 id="9-安装warp-transducer"><a href="#9-安装warp-transducer" class="headerlink" title="9.安装warp-transducer"></a>9.安装warp-transducer</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs plain">./install_warp-transducer.sh<br></code></pre></td></tr></table></figure><p>（非必要安装，可以用的时候再安装）如果出错，把步骤5重新运行一遍。</p><h1 id="10-运行以下yesno"><a href="#10-运行以下yesno" class="headerlink" title="10.运行以下yesno"></a>10.运行以下yesno</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs plain">cd ~/espnet/egs/yesno/asr1<br>./run<br></code></pre></td></tr></table></figure><p>出现以下结果说明安装正确</p><p><img src="/../images/ESPnet%E5%AE%89%E8%A3%85/image%20(5).png"></p>]]></content>
    
    
    <categories>
      
      <category>espnet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>espnet</tag>
      
      <tag>linux</tag>
      
      <tag>安装</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>因为德芙的第一篇文章</title>
    <link href="/2022/04/12/hello-world/"/>
    <url>/2022/04/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>嘻嘻嘻嘻，今天是4月12号，夏冬疯狂交替的日子，但是由于目前全国疫情又到处爆发了，大家还是要小心注意。</p><p>这里是我的个人博客，这是我在博客上写的第一篇文章，主要是欢迎各位小伙伴的到来。</p><p>说起来，为什么突然想搞个博客呢，因为就是突然想搞个博客，嘻嘻，以前都是用石墨文档写学习记录给自己看，现在想用博客记录自己的学习记录（主要是自己学习和吸收的一个过程，当然可能会有错误，欢迎大家指正）。但是我还是个在校学生，可能更新频率会低一点，还请见谅。</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
